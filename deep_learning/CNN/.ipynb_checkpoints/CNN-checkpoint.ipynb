{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This page will introduce the basic knowledge about CNN, with the implementation in tensorflow. The simple flow is shown below:\n",
    "\n",
    "<img src=\"simple_CNN_flow.png\" width=\"70%\">\n",
    "\n",
    "In this flow, we list the paramters as flows:\n",
    "\n",
    "<img src=\"paramter_in_each_layer.png\" width=\"70%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each CNN layer, we give the related tensorflow functions:\n",
    "convolution layer:      tf.nn.conv2d\n",
    "activation layer:       tf.nn.relu\n",
    "pooling layer:          tf.nn.max_pool\n",
    "full-connected layer:   tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. conv2d usage\n",
    "\n",
    "filter is [filter_height,filter_width,in_channels,out_channels]：height, width, image channels, num of filters. Note that it is completely different from input as [batch,in_height,in_width,in_channels]\n",
    "It is very hard to understand that the conversion of 4-D filter. Here lists some figures to elaborate this conversion.\n",
    "\n",
    "<img src=\"multi_D_filter.png\" width=\"70%\">\n",
    "\n",
    "\n",
    "The filter matrix:\n",
    "<img src=\"4D_filter.png\" width=\"60%\">\n",
    "\n",
    "One red column means one filter, so two columns mean two filters. The green and yellow columns mean two channels. (very hard to understand the dim conversion)\n",
    "\n",
    "The result:\n",
    "<img src=\"conv2D_result.png\" width=\"60%\">\n",
    "\n",
    "\n",
    "Same here, one red column means one filter result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data: \n",
      "[[[[0. 1.]\n",
      "   [1. 2.]]\n",
      "\n",
      "  [[3. 2.]\n",
      "   [1. 2.]]]]\n",
      "filter: \n",
      "[[[[1. 1.]\n",
      "   [1. 1.]]\n",
      "\n",
      "  [[0. 0.]\n",
      "   [0. 0.]]]]\n",
      "convolution: \n",
      "tf.Tensor(\n",
      "[[[[1. 1.]\n",
      "   [3. 3.]]\n",
      "\n",
      "  [[5. 5.]\n",
      "   [3. 3.]]]], shape=(1, 2, 2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import numpy as np\n",
    "tfe.enable_eager_execution()\n",
    "\n",
    "# input data\n",
    "data = np.array([0, 1, 1, 2, 3, 2, 1, 2], dtype=np.float32)\n",
    "#reshaped_data = data.reshape(data, [2, 2, 2, 2])\n",
    "reshaped_data = data.reshape([1, 2, 2, 2])\n",
    "\n",
    "print(\"input data: \")\n",
    "print(reshaped_data)\n",
    "\n",
    "# filter\n",
    "filter = np.array([1, 1, 1, 1, 0, 0, 0, 0], dtype=np.float32)\n",
    "reshaped_filter = filter.reshape([1, 2, 2, 2])\n",
    "\n",
    "print(\"filter: \")\n",
    "print(reshaped_filter)\n",
    "\n",
    "# conv2d(input, filter, stribes, padding, use_cudnn_on_gpu(default true), data_format(NHWC(default) or NCHW), name)\n",
    "# N: number of batchs H:height W: weight C: channels\n",
    "convolution = tf.nn.conv2d(reshaped_data, reshaped_filter, [1, 1, 1, 1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "print(\"convolution: \")\n",
    "print(convolution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. sigmoid and Tanh usage\n",
    "-> sigmoid(x) = 1/(1+e^(-x))\n",
    "-> Tanh(x) = Sinh(x)/Conh(x) = (e^(x) - e^(-x))/(e^(x) + e^(-x)) = 2 sigmoid(2x)-1\n",
    "-> ReLU: f(y)=0 when y<0 f(y)=y when y>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.26894143 0.5        0.7310586  0.8807971 ], shape=(4,), dtype=float32)\n",
      "tf.Tensor([-0.7615942  0.         0.7615942  0.9640276], shape=(4,), dtype=float32)\n",
      "tf.Tensor([0. 0. 1. 2.], shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import numpy as np\n",
    "tfe.enable_eager_execution()\n",
    "\n",
    "x = tf.range(-1, 3)\n",
    "y = tf.to_float(x)\n",
    "\n",
    "z = tf.sigmoid(y)\n",
    "\n",
    "print(z)\n",
    "\n",
    "m = tf.tanh(y)\n",
    "\n",
    "print(m)\n",
    "\n",
    "n = tf.nn.relu(y)\n",
    "print(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. dropout usage\n",
    "-> keep_prob: keep prob, and the left elements should multiply 1/keep_prob\n",
    "-> noise_shape: if the input data is [k,l,m,n], the noise shape is [k,1,1,n], then the data should dropout at first dimension and fourth dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0707 15:21:10.445700 10624 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0707 15:21:10.445700 10624 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[12. 11. 10.  9.]\n",
      " [ 8.  7.  6.  5.]\n",
      " [ 4.  3.  2.  1.]\n",
      " [-1. -2. -3. -4.]], shape=(4, 4), dtype=float32) (4, 4)\n",
      "#####random dropout#######\n",
      "tf.Tensor(\n",
      "[[  0.  55.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [ -0. -10.  -0.  -0.]], shape=(4, 4), dtype=float32)\n",
      "#####first dim############\n",
      "tf.Tensor(\n",
      "[[  0.    0.    0.    0. ]\n",
      " [  0.    0.    0.    0. ]\n",
      " [  0.    0.    0.    0. ]\n",
      " [ -2.5  -5.   -7.5 -10. ]], shape=(4, 4), dtype=float32)\n",
      "#####second dim###########\n",
      "tf.Tensor(\n",
      "[[  0.    0.   25.   22.5]\n",
      " [  0.    0.   15.   12.5]\n",
      " [  0.    0.    5.    2.5]\n",
      " [ -0.   -0.   -7.5 -10. ]], shape=(4, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import numpy as np\n",
    "tfe.enable_eager_execution()\n",
    "\n",
    "a = tf.constant([\n",
    "                    [12, 11, 10, 9],\n",
    "                    [8, 7, 6, 5],\n",
    "                    [4, 3, 2, 1],\n",
    "                    [-1, -2, -3, -4]\n",
    "])\n",
    "\n",
    "a = tf.to_float(a)\n",
    "print(a, a.shape)\n",
    "\n",
    "print(\"#####random dropout#######\")\n",
    "print(tf.nn.dropout(a, 0.2))\n",
    "\n",
    "print(\"#####first dim############\")\n",
    "print(tf.nn.dropout(a, 0.4, noise_shape=[4, 1]))\n",
    "\n",
    "print(\"#####second dim###########\")\n",
    "print(tf.nn.dropout(a, 0.4, noise_shape=[1, 4]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. max_pool/avg_pool usage\n",
    "-> ksize: [1, height, width, 1] pooling window size\n",
    "-> strides: [1, stride, stride, 1]\n",
    "-> padding: valid, same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 4, 1) (1, 2, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import numpy as np\n",
    "tfe.enable_eager_execution()\n",
    "\n",
    "a = tf.constant([\n",
    "                  [\n",
    "                    [[12], [11], [10], [9]],\n",
    "                    [[8], [7], [6], [5]],\n",
    "                    [[4], [3], [2], [1]],\n",
    "                    [[-1], [-2], [-3], [-4]]\n",
    "                  ]\n",
    "                ])\n",
    "\n",
    "\n",
    "a = tf.to_float(a)\n",
    "#print(a, a.shape)\n",
    "\n",
    "\n",
    "b = tf.nn.max_pool(a, [1,2,2,1], [1,2,2,1], padding=\"VALID\")\n",
    "\n",
    "print(a.shape, b.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization:\n",
    "是为了克服神经网络层数加深导致难以训练而诞生的一个算法。根据ICS理论，当训练集的样本数据和目标样本集分布不一致的时候，训练得到的模型无法很好的泛化。\n",
    "而在神经网络中，每一层的输入在经过层内操作之后必然会导致与原来对应的输入信号分布不同,并且前层神经网络的增加会被后面的神经网络不对的累积放大。这个问题的一个解决思路就是根据训练样本与目标样本的比例对训练样本进行一个矫正，而BN算法（批标准化）则可以用来规范化某些层或者所有层的输入，从而固定每层输入信号的均值与方差。\n",
    "\n",
    "If no normalization, when the layer becomes more, the data will lose the its effectiveness. So before the data going to next layer, we normalize it to make the data located in the sensitive area of activation function. Then data can be kept effective after many layers. \n",
    "\n",
    "<img src=\"batch_norm.jpg\" width=\"70%\">\n",
    "\n",
    "epsilon(ε) is a small number to avoid divider equals to zero.\n",
    "\n",
    "批标准化一般用在非线性映射（激活函数）之前，对y= Wx + b进行规范化，是结果(输出信号的各个维度)的均值都为0,方差为1,让每一层的输入有一个稳定的分布会有利于网络的训练\n",
    "<img src=\"batch_norm_layer.png\" width=\"50%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 3.8289744e-03 -5.6336448e-04  1.4799552e-03  5.7142577e-04\n",
      "  4.7703541e-04 -2.0087755e-04  6.9286919e-04  1.1301619e-03\n",
      "  1.3710989e-05 -8.5227739e-04  3.0588850e-03  1.3085750e-03\n",
      " -6.7887345e-04  1.5129700e-03  3.1190203e-03 -3.7281897e-03\n",
      " -2.3043153e-03  1.9120270e-03  1.3753374e-03 -1.5845909e-03\n",
      " -3.1477124e-03  1.1667183e-03  1.5678816e-06  4.1462742e-03\n",
      " -1.5225249e-03  1.5106871e-03  1.7305558e-03  5.9000710e-03\n",
      " -2.1053595e-03  1.3912013e-03 -1.8897259e-03 -3.6140513e-03\n",
      "  5.8038225e-03  3.0917746e-03 -4.6636704e-03  6.2597808e-03\n",
      " -2.1593431e-03  9.2474639e-04  1.4937918e-03  4.0106038e-03\n",
      " -4.3696724e-04 -2.2082906e-03  2.2996292e-03 -1.9521383e-04\n",
      "  3.8914609e-04  1.6991515e-04 -1.5982927e-03 -1.8002823e-03\n",
      " -2.5288896e-03  5.9729507e-03  1.8585534e-03  1.1368748e-03\n",
      " -3.2065357e-03 -3.6522895e-03 -4.1670967e-03  2.8582634e-03\n",
      "  1.3978590e-04 -2.0171325e-03  1.4095200e-03  5.9454900e-04\n",
      " -4.8744190e-04  8.1463857e-04 -1.3336164e-03 -1.9655142e-03], shape=(64,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[0.995723   0.9979959  1.0002259  0.9973978  0.99862623 0.9954173\n",
      " 1.0036516  0.99252284 1.0006188  1.0015666  0.9942094  0.9975402\n",
      " 1.002517   0.99981946 0.9966189  0.9969109  0.99722433 1.0036411\n",
      " 0.9986013  1.0014336  1.0030391  1.00569    1.00514    1.0019424\n",
      " 1.0037191  0.99882317 1.0113534  1.0037042  0.9977579  1.0038071\n",
      " 1.0029917  1.0004902  1.008821   1.0042913  0.99794734 1.0019531\n",
      " 0.9979969  1.0044556  1.004546   1.0041577  1.0010228  0.99969536\n",
      " 0.99943686 1.0016896  0.9992355  0.9991864  1.0039177  1.0030326\n",
      " 0.99723047 1.0011059  1.0120144  1.0016648  0.99581146 1.0048925\n",
      " 1.0109342  0.99618703 0.99979925 0.9948925  0.99893117 0.9996642\n",
      " 0.99763    1.006174   1.002819   0.9966446 ], shape=(64,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[ 6.91337109e-01 -1.71340442e+00 -9.13135767e-01 ...  8.63906145e-01\n",
      "     2.62799907e+00  7.70343900e-01]\n",
      "   [ 1.36153054e+00  1.03459895e-01  1.42550468e+00 ... -1.51438260e+00\n",
      "     1.38544512e+00  9.87711847e-01]\n",
      "   [-1.16111517e+00  1.32724166e+00  9.28657711e-01 ...  1.04448295e+00\n",
      "     9.37763274e-01  7.14210868e-02]\n",
      "   ...\n",
      "   [ 2.05968308e+00 -3.94993663e-01  1.21119583e+00 ...  1.16115928e+00\n",
      "     2.87799501e+00  6.32672191e-01]\n",
      "   [-7.62782395e-01  4.68920350e-01  2.06521034e-01 ...  1.87244439e+00\n",
      "     5.46680689e-01  2.38602221e-01]\n",
      "   [ 1.35310423e+00 -5.93130112e-01  4.43399251e-01 ...  4.57141459e-01\n",
      "     5.26651561e-01  6.01287246e-01]]\n",
      "\n",
      "  [[ 2.46586204e-02  1.90790272e+00  6.77201390e-01 ... -1.11879373e+00\n",
      "     2.07595611e+00  1.32927287e+00]\n",
      "   [ 1.77501941e+00  2.17140770e+00  1.60977924e+00 ...  1.23948348e+00\n",
      "     2.75655079e+00 -2.00902104e-01]\n",
      "   [-1.16228938e+00 -3.31533074e-01  3.10200453e-02 ...  1.73041987e+00\n",
      "     4.57983971e-01  8.35140407e-01]\n",
      "   ...\n",
      "   [ 1.13982320e+00  7.46982038e-01  5.71764886e-01 ...  1.45069325e+00\n",
      "     2.14947402e-01  2.22933173e-01]\n",
      "   [ 6.87369764e-01 -2.06170440e-01  1.72851253e+00 ...  1.77622890e+00\n",
      "     4.80299711e-01  1.57749403e+00]\n",
      "   [ 2.94798446e+00 -1.09466147e+00 -6.72126889e-01 ...  1.74948812e+00\n",
      "    -2.60926723e-01  2.00975227e+00]]\n",
      "\n",
      "  [[ 2.12290645e+00  2.94837475e+00  5.92298388e-01 ...  1.94341481e-01\n",
      "     2.51979184e+00  4.94193852e-01]\n",
      "   [-1.93266690e-01 -1.08414292e-01 -5.35818577e-01 ...  2.24060059e+00\n",
      "     2.40077519e+00 -1.24619007e-02]\n",
      "   [ 1.59137321e+00  1.57256424e-01  2.71986556e+00 ...  1.55934918e+00\n",
      "     5.11247754e-01  2.12967587e+00]\n",
      "   ...\n",
      "   [ 3.92712653e-01  3.29439211e+00  1.19440496e+00 ...  1.32334399e+00\n",
      "     8.55712295e-01  1.74988914e+00]\n",
      "   [ 2.37048674e+00  1.64519405e+00  1.85069609e+00 ...  5.49729586e-01\n",
      "     8.14951956e-01 -2.86683559e-01]\n",
      "   [ 5.15066862e-01  6.32091522e-01  1.59734201e+00 ...  5.33850789e-01\n",
      "     3.03627133e+00  2.00964594e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-1.23408747e+00  9.12436485e-01  5.73398948e-01 ...  1.66914845e+00\n",
      "     1.30868602e+00  9.74093258e-01]\n",
      "   [ 1.12378037e+00  1.82290816e+00  7.20975697e-01 ...  1.98452520e+00\n",
      "     1.65172315e+00 -2.10954189e-01]\n",
      "   [ 1.68588614e+00  3.07356310e+00  1.51858461e+00 ...  7.69154131e-01\n",
      "     1.51394105e+00  1.46378171e+00]\n",
      "   ...\n",
      "   [-7.84158647e-01 -8.57626081e-01 -6.81861043e-01 ...  2.53596520e+00\n",
      "    -3.09556246e-01 -8.76326680e-01]\n",
      "   [ 2.20361531e-01  8.07295442e-01  8.45365822e-01 ...  3.49594760e+00\n",
      "     1.38147068e+00  1.71134567e+00]\n",
      "   [ 5.58271527e-01  9.40309346e-01  1.97832704e+00 ...  1.16867769e+00\n",
      "     1.94476354e+00 -2.52730846e-02]]\n",
      "\n",
      "  [[ 1.81094635e+00  2.13837814e+00 -4.87301588e-01 ...  2.80640244e+00\n",
      "     2.00864029e+00 -7.13899851e-01]\n",
      "   [ 1.33819091e+00  6.58955932e-01  7.89556324e-01 ...  6.05678201e-01\n",
      "     1.76083624e+00  1.32427049e+00]\n",
      "   [ 3.62445951e-01  9.45133030e-01  6.77265286e-01 ...  7.93752015e-01\n",
      "     8.04097235e-01  1.05283546e+00]\n",
      "   ...\n",
      "   [ 1.20964468e+00  1.18811655e+00  1.43221688e+00 ... -6.94940031e-01\n",
      "     1.28335929e+00  7.51883626e-01]\n",
      "   [ 3.13620877e+00  6.45664811e-01 -3.93620610e-01 ...  8.14358592e-02\n",
      "     6.13640368e-01  1.69737816e+00]\n",
      "   [-5.20402193e-03  1.89415216e+00  1.17688215e+00 ...  1.39382899e+00\n",
      "     1.98751402e+00  1.06619895e-01]]\n",
      "\n",
      "  [[-8.55215967e-01  1.84689808e+00  1.18646705e+00 ...  4.17015433e-01\n",
      "     1.41332150e+00  4.56643283e-01]\n",
      "   [ 4.20937777e-01 -6.12724543e-01  9.56829429e-01 ...  1.18036795e+00\n",
      "     5.72292805e-01  1.21666217e+00]\n",
      "   [ 9.16688323e-01  1.39195263e+00  1.91304219e+00 ...  9.08724487e-01\n",
      "     1.52310419e+00  2.05100000e-01]\n",
      "   ...\n",
      "   [ 4.59383547e-01 -1.42495394e-01  1.04879975e+00 ...  1.38898933e+00\n",
      "    -1.40282512e+00  8.96702886e-01]\n",
      "   [ 1.49285603e+00  1.61236584e-01  1.54380202e-02 ...  3.18551123e-01\n",
      "     3.09737265e-01  5.94201326e-01]\n",
      "   [ 1.42838657e+00  1.07676530e+00  1.28769338e+00 ...  1.24374950e+00\n",
      "     2.56186867e+00  1.26464844e-01]]]\n",
      "\n",
      "\n",
      " [[[ 1.97010684e+00  1.25488865e+00  1.63573551e+00 ...  9.00070369e-01\n",
      "     1.90086150e+00  1.74883342e+00]\n",
      "   [ 1.36226320e+00  1.73110271e+00  9.35134470e-01 ...  5.02852857e-01\n",
      "     2.03471088e+00  8.91265750e-01]\n",
      "   [ 7.58723140e-01  5.87822795e-02  1.73576713e+00 ...  3.29465508e-01\n",
      "    -6.93135262e-02 -8.71281981e-01]\n",
      "   ...\n",
      "   [ 1.63407302e+00  3.02332282e-01  9.02760029e-02 ...  1.50334239e-01\n",
      "    -8.42350006e-01  1.33200336e+00]\n",
      "   [ 1.92103064e+00  1.15980780e+00 -1.97714210e-01 ...  2.86174417e-01\n",
      "    -6.07023120e-01  9.52349722e-01]\n",
      "   [ 1.43545330e+00  2.20384932e+00  1.87923312e+00 ...  2.75010753e+00\n",
      "     1.45512879e+00  1.49966800e+00]]\n",
      "\n",
      "  [[-9.48400080e-01  1.77458787e+00  1.18242729e+00 ...  9.15945888e-01\n",
      "     2.12240696e+00  1.28770792e+00]\n",
      "   [ 8.41645360e-01  2.57238626e+00  2.05912018e+00 ... -4.42613959e-02\n",
      "     2.68260241e+00  2.87558317e-01]\n",
      "   [ 1.41610909e+00  3.55372846e-01  2.27664328e+00 ...  1.82977128e+00\n",
      "     1.20522535e+00  1.98432899e+00]\n",
      "   ...\n",
      "   [ 2.34904432e+00 -1.15108538e+00  1.43699324e+00 ...  2.26385736e+00\n",
      "     8.15203071e-01  9.27318633e-01]\n",
      "   [-1.73744738e-01  5.77756166e-01 -2.85612345e-02 ... -3.00067365e-01\n",
      "     1.64512157e-01  3.43785572e+00]\n",
      "   [ 1.11539304e-01  1.02720642e+00  1.66853833e+00 ...  8.98383737e-01\n",
      "     7.92926788e-01  1.25625885e+00]]\n",
      "\n",
      "  [[ 9.52230692e-01  1.90786195e+00  2.61777067e+00 ...  8.22156310e-01\n",
      "     1.44365656e+00  2.54334092e-01]\n",
      "   [-1.84971738e+00 -4.33193445e-02  1.82153153e+00 ...  1.19595230e-01\n",
      "     2.18753624e+00  1.08419836e+00]\n",
      "   [ 1.28144491e+00  9.79531825e-01  5.56270480e-01 ...  1.52842855e+00\n",
      "     1.07655752e+00  3.89786780e-01]\n",
      "   ...\n",
      "   [-4.32613671e-01  2.57866859e-01 -3.35058689e-01 ...  1.68721008e+00\n",
      "     2.98246527e+00  2.04007328e-01]\n",
      "   [ 1.97063589e+00  2.63578701e+00  7.79242337e-01 ...  8.35985720e-01\n",
      "    -2.67014742e-01  1.06819808e+00]\n",
      "   [ 1.53819203e+00  8.32215667e-01  4.37610197e+00 ...  1.19597316e-02\n",
      "     1.03687072e+00  2.82251263e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 1.00368690e+00  3.08827460e-01  1.38015926e-01 ...  1.55332041e+00\n",
      "     1.41746449e+00 -6.20252371e-01]\n",
      "   [ 1.19918442e+00  1.04687595e+00  2.19462276e-01 ...  9.47852015e-01\n",
      "     4.80053961e-01 -1.74759626e-02]\n",
      "   [ 1.27572370e+00 -1.09260607e+00  2.54330683e+00 ...  4.95756090e-01\n",
      "     1.11271656e+00  3.92028868e-01]\n",
      "   ...\n",
      "   [ 6.64946437e-01  1.65280271e+00 -3.90276790e-01 ...  1.05534256e-01\n",
      "     1.54121292e+00  2.25675249e+00]\n",
      "   [ 1.32129502e+00  1.19206405e+00  2.96147394e+00 ...  3.24902248e+00\n",
      "     9.42755044e-01  1.04927468e+00]\n",
      "   [ 1.32441378e+00  1.52018142e+00  6.45490885e-01 ...  1.43241584e+00\n",
      "     1.37663651e+00  3.18517733e+00]]\n",
      "\n",
      "  [[-9.16768014e-01  1.66651535e+00  5.70547700e-01 ... -6.93962038e-01\n",
      "     1.35471153e+00  2.87492096e-01]\n",
      "   [ 1.44334626e+00  3.78922439e+00  1.52307642e+00 ...  1.72218764e+00\n",
      "     1.91044772e+00  1.39650595e+00]\n",
      "   [-1.95957482e-01  1.22840762e+00  2.42799258e+00 ... -9.77167487e-02\n",
      "     1.75505376e+00 -2.56837606e-02]\n",
      "   ...\n",
      "   [-3.47193658e-01  2.01078033e+00 -3.97808075e-01 ...  1.87996733e+00\n",
      "     2.10010576e+00  4.63505089e-01]\n",
      "   [ 6.06778860e-02 -1.86432958e-01  1.03051603e+00 ... -4.18886125e-01\n",
      "     4.46787000e-01  1.03926146e+00]\n",
      "   [ 7.92586803e-03  1.82057714e+00  1.55177927e+00 ...  1.27503717e+00\n",
      "     7.85737157e-01  3.37039328e+00]]\n",
      "\n",
      "  [[ 1.36017442e+00  1.58097696e+00  1.91911292e+00 ...  5.47783136e-01\n",
      "     1.96831989e+00 -6.14878654e-01]\n",
      "   [ 1.06362581e+00  3.90184999e-01  2.01789916e-01 ...  7.02873290e-01\n",
      "    -5.42235374e-03  1.97597206e+00]\n",
      "   [ 1.25336707e+00  2.23297143e+00  8.19442451e-01 ...  1.70887125e+00\n",
      "     1.35557377e+00  2.59217691e+00]\n",
      "   ...\n",
      "   [ 9.92072165e-01  2.50471771e-01  6.97263956e-01 ...  1.76805723e+00\n",
      "     1.13052738e+00  9.29845452e-01]\n",
      "   [ 3.46868730e+00 -4.58510637e-01  6.11023307e-02 ...  2.92352772e+00\n",
      "     2.43068266e+00  1.36374545e+00]\n",
      "   [-3.48451734e-02  1.02570426e+00  1.90697026e+00 ...  9.14373457e-01\n",
      "    -3.39242220e-02  7.71680355e-01]]]\n",
      "\n",
      "\n",
      " [[[-2.35870481e-02  1.28867328e-01 -1.33749521e+00 ...  4.79984522e-01\n",
      "     6.58969998e-01  7.35763490e-01]\n",
      "   [ 9.59186077e-01 -2.18936563e-01  1.51139307e+00 ... -1.15921903e+00\n",
      "     2.17143440e+00  1.00391984e-01]\n",
      "   [ 4.35647964e-02  1.13552153e+00  9.46438670e-01 ...  1.45889974e+00\n",
      "     9.43900168e-01 -3.90701890e-01]\n",
      "   ...\n",
      "   [ 1.18601310e+00  2.28232718e+00  7.74094164e-01 ... -8.06325674e-03\n",
      "     3.47893524e+00  6.45033062e-01]\n",
      "   [ 2.44242549e-02  1.16611505e+00 -4.02891278e-01 ... -1.37544453e-01\n",
      "     8.67829323e-02  1.29939389e+00]\n",
      "   [ 1.69889176e+00 -3.96032691e-01  6.34191096e-01 ...  3.09569979e+00\n",
      "    -6.34258509e-01  4.94181037e-01]]\n",
      "\n",
      "  [[-9.14236486e-01  5.33943176e-01 -4.36693072e-01 ...  3.55324328e-01\n",
      "     1.15000498e+00  2.05214238e+00]\n",
      "   [ 2.95865655e+00  1.08147538e+00  1.00546658e+00 ...  2.33150935e+00\n",
      "     1.77955556e+00  3.57448959e+00]\n",
      "   [ 5.15127659e-01  1.93152285e+00  2.14338255e+00 ...  1.57739270e+00\n",
      "     1.26656389e+00  6.38468862e-01]\n",
      "   ...\n",
      "   [ 2.11209702e+00  1.25944924e+00  1.32202387e+00 ...  1.41282701e+00\n",
      "     8.48862767e-01  2.54504263e-01]\n",
      "   [ 9.61097896e-01  1.69442010e+00  7.62416303e-01 ...  1.13196337e+00\n",
      "     1.68021858e-01  1.34146225e+00]\n",
      "   [ 2.95136595e+00  1.87437773e+00 -4.74214435e-01 ...  1.58623195e+00\n",
      "     2.73690462e-01  1.69883347e+00]]\n",
      "\n",
      "  [[ 4.49080646e-01  2.04223776e+00  2.99667239e-01 ... -6.33299887e-01\n",
      "     2.62411737e+00  2.25189590e+00]\n",
      "   [ 2.60007954e+00  1.27430952e+00  1.90566540e+00 ...  1.39545059e+00\n",
      "     7.49484777e-01  1.77586961e+00]\n",
      "   [ 2.47381353e+00  2.05696917e+00  2.30922651e+00 ...  7.33129382e-01\n",
      "     3.41431439e-01  7.91221857e-01]\n",
      "   ...\n",
      "   [ 1.75026655e-01  1.19418252e+00 -6.24277830e-01 ...  4.88272905e-01\n",
      "    -1.44508481e-01  5.96729636e-01]\n",
      "   [ 4.65625525e-01 -1.02845240e+00  1.25253201e-03 ...  1.57038212e+00\n",
      "     1.25708067e+00  1.03137589e+00]\n",
      "   [ 1.60727811e+00  9.60025847e-01  1.08538473e+00 ...  1.03102911e+00\n",
      "    -5.90522289e-02  9.42859232e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-3.71764302e-02 -1.01469803e+00  2.81903207e-01 ...  1.30370808e+00\n",
      "     2.02290273e+00  4.78465974e-01]\n",
      "   [ 1.33686423e+00  1.27318025e+00  1.72958553e-01 ...  1.99720991e+00\n",
      "     3.25314760e+00  1.22312188e-01]\n",
      "   [ 1.35921669e+00  2.52826548e+00  1.97784114e+00 ...  4.30690646e-01\n",
      "     1.81157935e+00  2.64942217e+00]\n",
      "   ...\n",
      "   [ 1.98571181e+00  1.26261711e+00  1.94513023e+00 ...  2.63376355e+00\n",
      "     1.29087281e+00 -3.70051980e-01]\n",
      "   [ 1.07152438e+00  6.21484756e-01  2.04858541e+00 ...  3.02635550e+00\n",
      "     1.35677493e+00  7.09433675e-01]\n",
      "   [ 7.89372087e-01  1.72062886e+00  1.44919753e+00 ...  1.73929691e+00\n",
      "     1.03081775e+00  1.56135035e+00]]\n",
      "\n",
      "  [[ 2.36054587e+00 -1.28994703e-01  1.00773358e+00 ...  5.79150140e-01\n",
      "     2.58954287e-01  1.11442006e+00]\n",
      "   [ 7.07976580e-01 -5.70961475e-01 -1.86918259e-01 ...  1.73108578e+00\n",
      "    -3.95318389e-01  1.22263122e+00]\n",
      "   [ 2.14085031e+00 -1.60411954e-01 -1.93443072e+00 ...  2.70569324e-03\n",
      "     3.09298337e-01 -3.08158278e-01]\n",
      "   ...\n",
      "   [ 1.71386552e+00 -4.48596597e-01  1.26787710e+00 ...  1.27993417e+00\n",
      "     1.47555888e-01  6.13492370e-01]\n",
      "   [ 1.21608794e+00  1.20697188e+00  1.40043461e+00 ...  6.75828815e-01\n",
      "     7.03723252e-01  9.87486839e-01]\n",
      "   [-1.35884583e-01  1.45522892e+00  1.07345092e+00 ...  1.37620449e+00\n",
      "     1.93063104e+00  1.47835970e-01]]\n",
      "\n",
      "  [[ 1.10724795e+00  1.68317056e+00  1.11208892e+00 ...  2.56190395e+00\n",
      "     1.30053139e+00  4.76352990e-01]\n",
      "   [ 9.26706791e-01  2.09984016e+00  1.21717381e+00 ...  3.18675518e-01\n",
      "     8.43741894e-01  7.85464644e-01]\n",
      "   [ 2.49657297e+00  8.45679998e-01  3.67987275e-01 ...  6.96150064e-02\n",
      "     2.45738328e-01  1.84194732e+00]\n",
      "   ...\n",
      "   [ 1.14373088e+00  1.56286120e+00  2.85394263e+00 ...  5.65244973e-01\n",
      "     1.02748382e+00  1.81008863e+00]\n",
      "   [ 1.02329004e+00  9.29917455e-01  1.08843386e+00 ...  1.37355769e+00\n",
      "     1.45386100e+00  2.13679218e+00]\n",
      "   [ 8.44712675e-01  5.99656105e-01  2.02935600e+00 ...  1.60346687e+00\n",
      "     1.22924984e+00  1.57018352e+00]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 2.44617510e+00 -1.73199058e-01  1.93258977e+00 ...  2.82747293e+00\n",
      "     8.72501135e-02  3.64716828e-01]\n",
      "   [ 6.04572058e-01  5.24096727e-01  4.65346575e-01 ...  5.55677056e-01\n",
      "     2.38040781e+00  1.89789963e+00]\n",
      "   [ 8.06493402e-01  1.48784113e+00  5.23307085e-01 ...  1.51213932e+00\n",
      "     3.20251966e+00  1.66506672e+00]\n",
      "   ...\n",
      "   [-7.69366443e-01  5.70497036e-01  1.53413093e+00 ...  2.19882941e+00\n",
      "     3.42816138e+00 -2.46597886e-01]\n",
      "   [ 1.68452477e+00 -6.79054260e-01  2.39540601e+00 ... -6.49501979e-01\n",
      "     2.97567511e+00  1.28866434e-02]\n",
      "   [ 1.17600071e+00  1.69480824e+00  9.21885252e-01 ...  1.81753147e+00\n",
      "     1.75469613e+00  2.61266804e+00]]\n",
      "\n",
      "  [[ 3.36496282e+00  5.61630726e-01 -1.57250941e+00 ...  1.31010711e-01\n",
      "     1.44765139e+00 -1.01432812e+00]\n",
      "   [ 1.12691426e+00  1.50637627e-02  1.65235519e+00 ...  1.11746073e+00\n",
      "     1.62406993e+00  1.93163395e+00]\n",
      "   [ 1.43646789e+00  1.74865973e+00  1.10519159e+00 ...  2.18833089e-02\n",
      "     1.53117967e+00  1.30810368e+00]\n",
      "   ...\n",
      "   [ 3.17799425e+00  7.74774253e-01  1.59272707e+00 ... -3.59873116e-01\n",
      "    -3.20769787e-01 -9.75740910e-01]\n",
      "   [ 2.91237772e-01 -2.42277026e-01  6.69690251e-01 ...  2.32811689e+00\n",
      "     1.96866679e+00  2.60698795e-01]\n",
      "   [ 2.56872678e+00  2.37315595e-01  1.80013704e+00 ...  3.62480521e-01\n",
      "    -4.91804123e-01  7.69473255e-01]]\n",
      "\n",
      "  [[ 2.77805924e-02  1.81713700e-02  1.66407943e+00 ...  9.59808588e-01\n",
      "     5.73474109e-01  1.58442378e+00]\n",
      "   [ 2.08018303e+00  5.41146874e-01  9.18571591e-01 ...  2.62119710e-01\n",
      "    -1.60288382e+00  2.79444504e+00]\n",
      "   [-5.75852215e-01  9.08348262e-01  4.19578969e-01 ...  1.35125899e+00\n",
      "     3.19700480e-01  2.58351278e+00]\n",
      "   ...\n",
      "   [ 2.15310454e-02  1.89838290e-01  1.85736346e+00 ...  6.74214244e-01\n",
      "     1.62063539e+00 -6.99106455e-02]\n",
      "   [ 1.89490426e+00  7.89197505e-01  3.42669487e-01 ...  1.01015902e+00\n",
      "     1.50848103e+00  3.02793086e-01]\n",
      "   [ 2.29171586e+00  1.88907981e-02  1.53720415e+00 ...  1.54936075e+00\n",
      "    -8.49575996e-02  4.04443741e-02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-6.59589171e-02  3.66505563e-01  4.52510297e-01 ... -3.61892581e-02\n",
      "     1.03655124e+00  1.19294262e+00]\n",
      "   [ 9.27251101e-01  2.88024926e+00 -2.60125637e-01 ...  2.90382099e+00\n",
      "     9.96777236e-01  6.29962981e-01]\n",
      "   [ 8.64499927e-01  2.65179968e+00 -2.33058929e-01 ...  2.50303769e+00\n",
      "    -1.62767053e-01  1.29560924e+00]\n",
      "   ...\n",
      "   [-8.56868625e-02 -1.47399855e+00  4.45401967e-01 ...  2.69502306e+00\n",
      "     3.95134330e-01  4.46498632e-01]\n",
      "   [ 5.42054892e-01  1.49159050e+00  1.05496871e+00 ...  5.51190317e-01\n",
      "    -2.21680403e-01  1.02278447e+00]\n",
      "   [ 6.07091188e-01 -1.14526272e-01  6.87552869e-01 ...  1.02967572e+00\n",
      "     9.04822767e-01 -9.40572500e-01]]\n",
      "\n",
      "  [[ 2.09578133e+00  5.48940182e-01  3.91696453e-01 ...  7.59947062e-01\n",
      "     2.36832952e+00  2.35510564e+00]\n",
      "   [ 2.39173412e-01  2.43198776e+00 -1.30713165e+00 ... -8.39979053e-02\n",
      "    -2.27695465e-01  7.80071020e-02]\n",
      "   [ 4.93029714e-01  1.43126488e-01  4.98526841e-01 ...  7.76246011e-01\n",
      "     8.55475307e-01  1.34842539e+00]\n",
      "   ...\n",
      "   [ 9.55891013e-01  1.90418422e+00  2.66033530e+00 ...  6.52715027e-01\n",
      "     4.43334579e-01 -6.45209551e-02]\n",
      "   [ 5.75793266e-01  1.32731187e+00  5.88947177e-01 ...  4.61019576e-01\n",
      "     2.32071972e+00 -7.09028244e-02]\n",
      "   [ 1.90234447e+00  3.09036911e-01  1.20379078e+00 ...  5.13051689e-01\n",
      "     1.28022194e+00  2.95413136e-02]]\n",
      "\n",
      "  [[ 2.73922205e-01  2.64429617e+00 -4.60074067e-01 ...  2.01739287e+00\n",
      "     1.89106810e+00 -1.39784682e+00]\n",
      "   [ 2.30382037e+00  2.31525850e+00  2.52172422e+00 ...  4.96552169e-01\n",
      "     2.49155712e+00  1.98771119e+00]\n",
      "   [ 7.83933401e-02  9.76893961e-01  2.55877209e+00 ... -1.42997801e-01\n",
      "     2.05963469e+00  1.43415093e+00]\n",
      "   ...\n",
      "   [ 1.25673962e+00  1.88878965e+00  2.53474760e+00 ...  6.81888223e-01\n",
      "     5.30277491e-01  1.20376372e+00]\n",
      "   [-5.55025518e-01 -1.09100533e+00 -9.13125992e-01 ... -2.12909579e-02\n",
      "     7.59084821e-02  1.32229173e+00]\n",
      "   [ 2.63394165e+00  1.59281075e-01  1.44706023e+00 ...  7.42232144e-01\n",
      "     2.05774927e+00  2.00207758e+00]]]\n",
      "\n",
      "\n",
      " [[[ 6.97749019e-01  3.70211542e-01  9.13078547e-01 ...  1.10532725e+00\n",
      "     1.83106256e+00  3.71775210e-01]\n",
      "   [ 1.53323734e+00 -1.49173355e+00  1.14868903e+00 ...  2.23482609e+00\n",
      "     2.27700424e+00  1.69418108e+00]\n",
      "   [ 2.83174229e+00  7.54040837e-01  2.45860147e+00 ...  1.76325345e+00\n",
      "     7.60784030e-01  6.72170162e-01]\n",
      "   ...\n",
      "   [ 9.56926823e-01  1.72703826e+00  9.31333601e-01 ...  1.57828355e+00\n",
      "    -2.07232952e-01  1.56597197e-01]\n",
      "   [ 1.42391253e+00  3.37039530e-01  1.67060947e+00 ...  5.05266309e-01\n",
      "     9.88344848e-01 -8.54466438e-01]\n",
      "   [ 2.43336141e-01  9.40814614e-01  1.47662735e+00 ...  7.32420087e-02\n",
      "     1.25946784e+00  2.40281582e+00]]\n",
      "\n",
      "  [[ 2.07277369e+00  3.28023171e+00  2.22919846e+00 ...  1.17822170e+00\n",
      "     3.17270136e+00  1.33108997e+00]\n",
      "   [ 5.17816186e-01 -9.31646824e-02  1.84033608e+00 ...  1.17127776e+00\n",
      "     3.59782577e-01  7.36326218e-01]\n",
      "   [ 2.52114606e+00  2.67971420e+00  1.45262527e+00 ... -4.26135480e-01\n",
      "     9.81885195e-01  4.71353531e-04]\n",
      "   ...\n",
      "   [ 8.03485990e-01  9.21801209e-01  2.16368103e+00 ...  3.31701696e-01\n",
      "     5.03254414e-01  7.15750098e-01]\n",
      "   [ 7.06877112e-01 -2.59601474e-01  1.18489695e+00 ...  2.05111074e+00\n",
      "     1.48503900e-01  1.98141873e+00]\n",
      "   [ 1.67001128e+00  6.29401445e-01 -5.16167164e-01 ...  6.47319019e-01\n",
      "     1.85448956e+00  1.76319528e+00]]\n",
      "\n",
      "  [[ 2.80930424e+00  1.50260007e+00 -3.18612218e-01 ...  2.75314689e-01\n",
      "     1.19422019e+00  2.30933380e+00]\n",
      "   [ 1.05446672e+00  1.26227438e+00  1.48411405e+00 ...  2.87657678e-01\n",
      "     2.04810286e+00  8.33723843e-01]\n",
      "   [ 6.68433845e-01  1.58229995e+00 -3.04735780e-01 ...  7.36791849e-01\n",
      "     1.60793734e+00  8.69531870e-01]\n",
      "   ...\n",
      "   [ 1.55596280e+00  1.65659332e+00  1.69813967e+00 ...  5.22258997e-01\n",
      "     2.53133392e+00  2.83773422e+00]\n",
      "   [ 1.32753730e-01  3.06691098e+00  1.35515153e+00 ...  3.31078529e-01\n",
      "    -4.83074546e-01  9.67995882e-01]\n",
      "   [-7.54160285e-02  1.56986618e+00  2.53989983e+00 ...  2.40335166e-01\n",
      "    -7.51023293e-02  1.92267895e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 7.88178325e-01 -6.92561269e-01  4.50634181e-01 ... -6.32204950e-01\n",
      "     1.12914109e+00  1.71605682e+00]\n",
      "   [ 9.47841823e-01  1.13846660e-01 -4.33746457e-01 ...  5.39664865e-01\n",
      "     1.46034110e+00  2.34664679e+00]\n",
      "   [ 2.02343178e+00  9.90501285e-01  8.19264174e-01 ...  6.03938282e-01\n",
      "     2.04397976e-01  1.27702558e+00]\n",
      "   ...\n",
      "   [-1.94348311e+00  4.60056305e-01 -1.20575428e-01 ...  1.41325402e+00\n",
      "     1.38117063e+00  2.24231899e-01]\n",
      "   [ 1.98552847e+00  2.38311052e+00  1.68248808e+00 ...  3.64328146e-01\n",
      "     1.15422332e+00  3.29837894e+00]\n",
      "   [ 3.44623506e-01  7.51939416e-01  1.08237028e+00 ...  1.73963726e-01\n",
      "     6.66802108e-01  1.27532303e-01]]\n",
      "\n",
      "  [[ 3.92091751e-01  1.75875425e+00  8.92141759e-01 ...  2.08214402e+00\n",
      "     7.76275396e-02  7.87875354e-01]\n",
      "   [ 1.47195542e+00  1.54420042e+00  2.38298774e-01 ...  1.47852695e+00\n",
      "     1.80483699e+00  1.19698119e+00]\n",
      "   [ 2.27414274e+00  3.66783082e-01  2.38833249e-01 ...  1.44301319e+00\n",
      "     4.18343127e-01  2.31434822e+00]\n",
      "   ...\n",
      "   [ 8.42447877e-01 -4.46216226e-01  7.07613945e-01 ...  7.63965905e-01\n",
      "     5.46818316e-01  1.57229972e+00]\n",
      "   [ 1.41855299e-01  6.69031382e-01 -1.71311736e-01 ...  2.50602388e+00\n",
      "     1.01459062e+00  1.74956799e-01]\n",
      "   [ 1.83238840e+00  2.74418116e-01  1.37673330e+00 ...  2.84762502e-01\n",
      "     1.80984139e+00 -1.84581518e-01]]\n",
      "\n",
      "  [[ 1.02697432e-01 -8.04179311e-01  6.08731389e-01 ...  4.02776432e+00\n",
      "     5.90229034e-01 -1.12637675e+00]\n",
      "   [-2.18860209e-01  2.25039601e+00  1.63534427e+00 ...  7.48260975e-01\n",
      "     2.10692859e+00  1.46563482e+00]\n",
      "   [-8.75370443e-01  4.92905080e-01 -1.87266827e-01 ...  1.82747483e-01\n",
      "     7.09655046e-01  2.68996382e+00]\n",
      "   ...\n",
      "   [ 2.00180507e+00  2.11791945e+00  1.04814661e+00 ...  2.28015447e+00\n",
      "    -2.85052896e-01 -1.49971962e-01]\n",
      "   [ 6.49180651e-01  1.98718739e+00  1.22550797e+00 ...  2.25522423e+00\n",
      "     3.03732753e-01 -1.09353900e-01]\n",
      "   [ 6.67624474e-01  3.12285066e+00  1.22532535e+00 ...  9.34257567e-01\n",
      "     2.22574973e+00  2.20507956e+00]]]\n",
      "\n",
      "\n",
      " [[[ 9.20745850e-01  1.42359257e+00  1.10748291e+00 ...  1.43918502e+00\n",
      "     8.57740760e-01  1.00288594e+00]\n",
      "   [ 1.13431513e+00  1.89875603e+00  4.40938187e+00 ...  1.00117433e+00\n",
      "     1.13640535e+00  1.47731590e+00]\n",
      "   [-3.08697820e-02 -2.22701120e+00  2.02171326e-01 ... -5.64073741e-01\n",
      "     1.10650444e+00  1.63429809e+00]\n",
      "   ...\n",
      "   [ 4.11784828e-01  1.34480393e+00  1.98182273e+00 ...  1.13112283e+00\n",
      "     1.44220877e+00  1.38425136e+00]\n",
      "   [ 1.57402682e+00  1.63495386e+00  7.11192608e-01 ...  1.16222775e+00\n",
      "     9.34327841e-01  9.74802852e-01]\n",
      "   [ 1.52293479e+00  2.32071781e+00  2.20811844e+00 ...  3.05363178e+00\n",
      "     1.46577716e+00  9.06091571e-01]]\n",
      "\n",
      "  [[ 1.20133162e-03  1.99344957e+00  9.97259498e-01 ...  9.06422734e-01\n",
      "     3.23886335e-01  1.66867471e+00]\n",
      "   [ 9.45279181e-01  1.45615649e+00  5.04452705e-01 ...  8.18112791e-01\n",
      "     9.31294918e-01  1.54634452e+00]\n",
      "   [ 1.32712889e+00  2.19276571e+00  1.73695660e+00 ...  7.33459830e-01\n",
      "     7.85581112e-01  5.16084433e-01]\n",
      "   ...\n",
      "   [ 1.42867339e+00  3.08474422e-01  1.31241417e+00 ...  4.35221016e-01\n",
      "    -2.58069634e-01  9.53372359e-01]\n",
      "   [ 7.67279863e-01  8.43379736e-01  7.69381821e-01 ... -3.21821392e-01\n",
      "     1.03998661e-01  5.65563440e-02]\n",
      "   [ 8.10596645e-01  1.07450724e+00  9.61602688e-01 ...  2.18182039e+00\n",
      "     3.10724282e+00  9.73253191e-01]]\n",
      "\n",
      "  [[ 1.98398232e-01  1.40484619e+00  1.58426046e+00 ...  1.00401545e+00\n",
      "     1.95579433e+00  3.25267887e+00]\n",
      "   [ 6.22783363e-01  2.39571619e+00  3.70545447e-01 ...  2.02017236e+00\n",
      "     3.11615992e+00  1.77282166e+00]\n",
      "   [ 2.91890562e-01  2.62286186e-01  2.94159412e-01 ... -2.37366736e-01\n",
      "     6.03903294e-01  1.52364850e+00]\n",
      "   ...\n",
      "   [ 8.84968221e-01 -7.63302088e-01  3.75590563e-01 ...  1.21212792e+00\n",
      "     1.51617396e+00  2.02570271e+00]\n",
      "   [-2.17088997e-01  3.42381716e+00  1.18245924e+00 ...  1.92794228e+00\n",
      "     2.59645998e-01  1.77608049e+00]\n",
      "   [-4.90967095e-01  1.47106493e+00  2.31922555e+00 ...  2.46920562e+00\n",
      "     2.61318946e+00  1.88031614e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 5.76743960e-01  1.81007242e+00  6.13408506e-01 ...  7.20179319e-01\n",
      "     3.38100195e-01  1.14156127e-01]\n",
      "   [ 9.50874746e-01  1.49054623e+00  1.98685813e+00 ...  3.43791544e-01\n",
      "     5.05667925e-01  1.16610849e+00]\n",
      "   [ 5.57879686e-01  1.03720725e+00 -4.01543379e-02 ... -5.56168854e-01\n",
      "     2.02251554e-01  1.34215403e+00]\n",
      "   ...\n",
      "   [ 6.17958963e-01  1.39723933e+00  2.78668308e+00 ...  9.87194777e-01\n",
      "    -4.75522757e-01  8.65070045e-01]\n",
      "   [ 7.16263652e-01  3.88668418e-01  1.37560034e+00 ... -9.20879245e-02\n",
      "     6.17421806e-01  1.12269998e+00]\n",
      "   [ 9.65672135e-02 -1.07604861e-01 -1.49833679e-01 ...  3.02230775e-01\n",
      "     2.16927862e+00  1.48962677e-01]]\n",
      "\n",
      "  [[ 3.42424440e+00 -1.24688983e-01  1.12452853e+00 ...  5.90232372e-01\n",
      "     1.94519258e+00 -6.12080097e-02]\n",
      "   [ 1.01126921e+00  1.18609393e+00  1.45692420e+00 ...  8.58906984e-01\n",
      "     2.04903936e+00 -8.64174843e-01]\n",
      "   [ 3.15574861e+00  2.20249939e+00  1.68986273e+00 ...  1.47192144e+00\n",
      "     3.33150148e+00  1.12884092e+00]\n",
      "   ...\n",
      "   [ 6.49804115e-01  6.84194386e-01  4.53812480e-01 ...  1.58113289e+00\n",
      "     4.19237971e-01  4.13640165e+00]\n",
      "   [ 2.56315649e-01  1.94003952e+00  2.21006346e+00 ...  7.04934478e-01\n",
      "     4.00238454e-01  2.04173923e+00]\n",
      "   [ 1.24807656e+00 -8.00454378e-01  6.64166808e-01 ...  2.14030862e+00\n",
      "     1.28768718e+00  1.51575792e+00]]\n",
      "\n",
      "  [[ 4.67806399e-01  1.78202343e+00  4.66524303e-01 ...  2.83552074e+00\n",
      "     6.28600955e-01  6.46033645e-01]\n",
      "   [ 9.36882913e-01  1.93215954e+00  7.42872119e-01 ...  2.48981786e+00\n",
      "     1.44536650e+00  5.71932077e-01]\n",
      "   [ 1.09081566e+00  4.50736344e-01  1.69705927e-01 ...  2.46584082e+00\n",
      "     6.82983875e-01  2.63877153e+00]\n",
      "   ...\n",
      "   [ 2.37896347e+00  6.81871235e-01  1.42506361e-01 ...  6.80607557e-02\n",
      "     1.28426433e-01  3.47033203e-01]\n",
      "   [ 2.15625048e+00  5.04090190e-01  7.07435787e-01 ...  1.40128946e+00\n",
      "     1.00869513e+00  1.02810740e-01]\n",
      "   [ 1.68443942e+00 -9.01402116e-01  8.03500295e-01 ...  6.36197209e-01\n",
      "     1.20336103e+00  1.54338205e+00]]]], shape=(128, 32, 32, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import numpy as np\n",
    "tfe.enable_eager_execution()\n",
    "\n",
    "\n",
    "img_shape= [128, 32, 32, 64]\n",
    "Wx_plus_b = tf.Variable(tf.random_normal(img_shape))\n",
    "axis = list(range(len(img_shape)-1)) # [0,1,2] \n",
    "wb_mean, wb_var = tf.nn.moments(Wx_plus_b, axis) # cal mean and variance\n",
    "\n",
    "print(wb_mean)\n",
    "print(wb_var)\n",
    "\n",
    "variance_epsilon =  0.001\n",
    "offset_B = tf.Variable(tf.ones([64]))\n",
    "scale_r = tf.Variable(tf.ones([64]))\n",
    "\n",
    "Wx_plus_b_normalized = tf.nn.batch_normalization(Wx_plus_b, wb_mean, wb_var, offset_B, scale_r, variance_epsilon)\n",
    "print(Wx_plus_b_normalized)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"batch_norm_example.png\" width=\"50%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can have a complete CNN example. We use CNN to do the MNIST classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step: 0 loss: 2.4305596351623535\n",
      "step: 1 loss: 2.605008602142334\n",
      "step: 2 loss: 2.4038338661193848\n",
      "step: 3 loss: 2.3041231632232666\n",
      "step: 4 loss: 2.376779079437256\n",
      "step: 5 loss: 2.3150153160095215\n",
      "step: 6 loss: 2.317183017730713\n",
      "step: 7 loss: 2.2991952896118164\n",
      "step: 8 loss: 2.2660367488861084\n",
      "step: 9 loss: 2.29044246673584\n",
      "step: 10 loss: 2.233370065689087\n",
      "step: 11 loss: 2.174989700317383\n",
      "step: 12 loss: 2.1273531913757324\n",
      "step: 13 loss: 2.019120693206787\n",
      "step: 14 loss: 1.9836868047714233\n",
      "step: 15 loss: 2.243748188018799\n",
      "step: 16 loss: 2.1470084190368652\n",
      "step: 17 loss: 1.798295259475708\n",
      "step: 18 loss: 1.791310429573059\n",
      "step: 19 loss: 2.3259949684143066\n",
      "step: 20 loss: 2.201274871826172\n",
      "step: 21 loss: 2.0483617782592773\n",
      "step: 22 loss: 1.892544150352478\n",
      "step: 23 loss: 2.1100966930389404\n",
      "step: 24 loss: 2.2011420726776123\n",
      "step: 25 loss: 1.9154363870620728\n",
      "step: 26 loss: 1.7511225938796997\n",
      "step: 27 loss: 2.1912710666656494\n",
      "step: 28 loss: 2.218104124069214\n",
      "step: 29 loss: 1.8276138305664062\n",
      "step: 30 loss: 1.376214623451233\n",
      "step: 31 loss: 1.4351208209991455\n",
      "step: 32 loss: 1.8228074312210083\n",
      "step: 33 loss: 2.1064724922180176\n",
      "step: 34 loss: 2.130190372467041\n",
      "step: 35 loss: 1.7186121940612793\n",
      "step: 36 loss: 1.2838157415390015\n",
      "step: 37 loss: 1.1956267356872559\n",
      "step: 38 loss: 2.2166807651519775\n",
      "step: 39 loss: 2.115192174911499\n",
      "step: 40 loss: 1.2184436321258545\n",
      "step: 41 loss: 1.1318262815475464\n",
      "step: 42 loss: 0.9668048620223999\n",
      "step: 43 loss: 0.8209651708602905\n",
      "step: 44 loss: 1.2547121047973633\n",
      "step: 45 loss: 1.634367823600769\n",
      "step: 46 loss: 1.3719784021377563\n",
      "step: 47 loss: 0.891074001789093\n",
      "step: 48 loss: 0.6632670760154724\n",
      "step: 49 loss: 0.9921826124191284\n",
      "step: 50 loss: 1.2989901304244995\n",
      "step: 51 loss: 1.3811744451522827\n",
      "step: 52 loss: 0.9096084833145142\n",
      "step: 53 loss: 0.6492996215820312\n",
      "step: 54 loss: 0.5549153685569763\n",
      "step: 55 loss: 0.4847436547279358\n",
      "step: 56 loss: 0.7376635670661926\n",
      "step: 57 loss: 0.5318566560745239\n",
      "step: 58 loss: 0.8246082067489624\n",
      "step: 59 loss: 2.2983057498931885\n",
      "step: 60 loss: 1.6843944787979126\n",
      "step: 61 loss: 1.4959936141967773\n",
      "step: 62 loss: 1.7013429403305054\n",
      "step: 63 loss: 1.3206485509872437\n",
      "step: 64 loss: 1.4546018838882446\n",
      "step: 65 loss: 0.926246166229248\n",
      "step: 66 loss: 0.6441847085952759\n",
      "step: 67 loss: 0.4886219799518585\n",
      "step: 68 loss: 0.4779469668865204\n",
      "step: 69 loss: 0.8286292552947998\n",
      "step: 70 loss: 0.8552545309066772\n",
      "step: 71 loss: 0.6848182678222656\n",
      "step: 72 loss: 0.462657630443573\n",
      "step: 73 loss: 0.45351356267929077\n",
      "step: 74 loss: 0.41219937801361084\n",
      "step: 75 loss: 0.32436397671699524\n",
      "step: 76 loss: 0.5345618724822998\n",
      "step: 77 loss: 0.7597827315330505\n",
      "step: 78 loss: 0.4330545663833618\n",
      "step: 79 loss: 0.39542704820632935\n",
      "step: 80 loss: 0.6869989633560181\n",
      "step: 81 loss: 0.9186850190162659\n",
      "step: 82 loss: 0.8072430491447449\n",
      "step: 83 loss: 0.5559329390525818\n",
      "step: 84 loss: 0.43774479627609253\n",
      "step: 85 loss: 0.35600346326828003\n",
      "step: 86 loss: 0.4517728388309479\n",
      "step: 87 loss: 1.0288732051849365\n",
      "step: 88 loss: 1.201529860496521\n",
      "step: 89 loss: 0.7099044919013977\n",
      "step: 90 loss: 0.5996388792991638\n",
      "step: 91 loss: 0.32981613278388977\n",
      "step: 92 loss: 0.6497511267662048\n",
      "step: 93 loss: 0.3550899624824524\n",
      "step: 94 loss: 0.4381426274776459\n",
      "step: 95 loss: 0.32287654280662537\n",
      "step: 96 loss: 0.26478224992752075\n",
      "step: 97 loss: 0.255380243062973\n",
      "step: 98 loss: 0.29162049293518066\n",
      "step: 99 loss: 0.2850620746612549\n",
      "step: 100 loss: 0.3140908479690552\n",
      "step: 101 loss: 0.22941617667675018\n",
      "step: 102 loss: 0.4256402254104614\n",
      "step: 103 loss: 0.32475602626800537\n",
      "step: 104 loss: 0.29521456360816956\n",
      "step: 105 loss: 0.26005393266677856\n",
      "step: 106 loss: 0.4821214973926544\n",
      "step: 107 loss: 0.5977776646614075\n",
      "step: 108 loss: 0.5521209836006165\n",
      "step: 109 loss: 0.4082467555999756\n",
      "step: 110 loss: 0.3307323157787323\n",
      "step: 111 loss: 0.19673360884189606\n",
      "step: 112 loss: 0.1790834665298462\n",
      "step: 113 loss: 0.23416465520858765\n",
      "step: 114 loss: 0.3025814890861511\n",
      "step: 115 loss: 0.5121946930885315\n",
      "step: 116 loss: 0.5295794010162354\n",
      "step: 117 loss: 0.19986377656459808\n",
      "step: 118 loss: 0.17155474424362183\n",
      "step: 119 loss: 0.18408218026161194\n",
      "step: 120 loss: 0.30329012870788574\n",
      "step: 121 loss: 0.29338696599006653\n",
      "step: 122 loss: 0.390040785074234\n",
      "step: 123 loss: 0.18974098563194275\n",
      "step: 124 loss: 0.20597253739833832\n",
      "step: 125 loss: 0.2141810953617096\n",
      "step: 126 loss: 0.3429194986820221\n",
      "step: 127 loss: 0.25894033908843994\n",
      "step: 128 loss: 0.17778293788433075\n",
      "step: 129 loss: 0.22315439581871033\n",
      "step: 130 loss: 0.11553489416837692\n",
      "step: 131 loss: 0.20284511148929596\n",
      "step: 132 loss: 0.16255629062652588\n",
      "step: 133 loss: 0.3186757564544678\n",
      "step: 134 loss: 0.14871124923229218\n",
      "step: 135 loss: 0.10674289613962173\n",
      "step: 136 loss: 0.22995327413082123\n",
      "step: 137 loss: 0.20977294445037842\n",
      "step: 138 loss: 0.21087875962257385\n",
      "step: 139 loss: 0.21985818445682526\n",
      "step: 140 loss: 0.21428388357162476\n",
      "step: 141 loss: 0.08524816483259201\n",
      "step: 142 loss: 0.20750360190868378\n",
      "step: 143 loss: 0.35922715067863464\n",
      "step: 144 loss: 0.16429875791072845\n",
      "step: 145 loss: 0.11362110823392868\n",
      "step: 146 loss: 0.22032834589481354\n",
      "step: 147 loss: 0.1900072544813156\n",
      "step: 148 loss: 0.30295661091804504\n",
      "step: 149 loss: 0.41445252299308777\n",
      "step: 150 loss: 0.24279336631298065\n",
      "step: 151 loss: 0.15096281468868256\n",
      "step: 152 loss: 0.11495128273963928\n",
      "step: 153 loss: 0.1201709732413292\n",
      "step: 154 loss: 0.14658969640731812\n",
      "step: 155 loss: 0.17119579017162323\n",
      "step: 156 loss: 0.18003539741039276\n",
      "step: 157 loss: 0.2519181966781616\n",
      "step: 158 loss: 0.20318172872066498\n",
      "step: 159 loss: 0.16582204401493073\n",
      "step: 160 loss: 0.20962947607040405\n",
      "step: 161 loss: 0.1809832602739334\n",
      "step: 162 loss: 0.10626138746738434\n",
      "step: 163 loss: 0.14233005046844482\n",
      "step: 164 loss: 0.21377398073673248\n",
      "step: 165 loss: 0.14478133618831635\n",
      "step: 166 loss: 0.19998402893543243\n",
      "step: 167 loss: 0.2062513530254364\n",
      "step: 168 loss: 0.21337634325027466\n",
      "step: 169 loss: 0.15356995165348053\n",
      "step: 170 loss: 0.3753937780857086\n",
      "step: 171 loss: 0.12020287662744522\n",
      "step: 172 loss: 0.2559128999710083\n",
      "step: 173 loss: 0.09277865290641785\n",
      "step: 174 loss: 0.10148805379867554\n",
      "step: 175 loss: 0.0847831517457962\n",
      "step: 176 loss: 0.11438778787851334\n",
      "step: 177 loss: 0.372570276260376\n",
      "step: 178 loss: 0.17848023772239685\n",
      "step: 179 loss: 0.2808888554573059\n",
      "step: 180 loss: 0.27383387088775635\n",
      "step: 181 loss: 0.218269482254982\n",
      "step: 182 loss: 0.1103193461894989\n",
      "step: 183 loss: 0.17600402235984802\n",
      "step: 184 loss: 0.1627958118915558\n",
      "step: 185 loss: 0.20565257966518402\n",
      "step: 186 loss: 0.09890909492969513\n",
      "step: 187 loss: 0.23137812316417694\n",
      "step: 188 loss: 0.16026896238327026\n",
      "step: 189 loss: 0.14594455063343048\n",
      "step: 190 loss: 0.17849396169185638\n",
      "step: 191 loss: 0.12269172817468643\n",
      "step: 192 loss: 0.10195531696081161\n",
      "step: 193 loss: 0.05439012497663498\n",
      "step: 194 loss: 0.08920320868492126\n",
      "step: 195 loss: 0.17152534425258636\n",
      "step: 196 loss: 0.07616952061653137\n",
      "step: 197 loss: 0.09954884648323059\n",
      "step: 198 loss: 0.205316424369812\n",
      "step: 199 loss: 0.22894257307052612\n",
      "step: 200 loss: 0.22586262226104736\n",
      "step: 201 loss: 0.5285375714302063\n",
      "step: 202 loss: 0.3310413062572479\n",
      "step: 203 loss: 0.38451507687568665\n",
      "step: 204 loss: 0.1554511934518814\n",
      "step: 205 loss: 0.12785348296165466\n",
      "step: 206 loss: 0.12514343857765198\n",
      "step: 207 loss: 0.1636541336774826\n",
      "step: 208 loss: 0.1732371300458908\n",
      "step: 209 loss: 0.15603967010974884\n",
      "step: 210 loss: 0.30684834718704224\n",
      "step: 211 loss: 0.11661276966333389\n",
      "step: 212 loss: 0.19036605954170227\n",
      "step: 213 loss: 0.2718605399131775\n",
      "step: 214 loss: 0.1245708018541336\n",
      "step: 215 loss: 0.2278003692626953\n",
      "step: 216 loss: 0.08355545997619629\n",
      "step: 217 loss: 0.12297481298446655\n",
      "step: 218 loss: 0.08419745415449142\n",
      "step: 219 loss: 0.1544521003961563\n",
      "step: 220 loss: 0.0991925299167633\n",
      "step: 221 loss: 0.14220492541790009\n",
      "step: 222 loss: 0.2774932384490967\n",
      "step: 223 loss: 0.21817392110824585\n",
      "step: 224 loss: 0.21307243406772614\n",
      "step: 225 loss: 0.14147499203681946\n",
      "step: 226 loss: 0.08262776583433151\n",
      "step: 227 loss: 0.13543492555618286\n",
      "step: 228 loss: 0.12919041514396667\n",
      "step: 229 loss: 0.15706929564476013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 230 loss: 0.055690936744213104\n",
      "step: 231 loss: 0.2711726129055023\n",
      "step: 232 loss: 0.11379889398813248\n",
      "step: 233 loss: 0.11739370971918106\n",
      "step: 234 loss: 0.07144855707883835\n",
      "step: 235 loss: 0.160944402217865\n",
      "step: 236 loss: 0.26357725262641907\n",
      "step: 237 loss: 0.10892410576343536\n",
      "step: 238 loss: 0.1572379171848297\n",
      "step: 239 loss: 0.19911348819732666\n",
      "step: 240 loss: 0.33268195390701294\n",
      "step: 241 loss: 0.4183018207550049\n",
      "step: 242 loss: 0.38179272413253784\n",
      "step: 243 loss: 0.20210213959217072\n",
      "step: 244 loss: 0.14793500304222107\n",
      "step: 245 loss: 0.16565610468387604\n",
      "step: 246 loss: 0.11878971755504608\n",
      "step: 247 loss: 0.23818595707416534\n",
      "step: 248 loss: 0.24248270690441132\n",
      "step: 249 loss: 0.15782903134822845\n",
      "step: 250 loss: 0.08866970986127853\n",
      "step: 251 loss: 0.2175481915473938\n",
      "step: 252 loss: 0.07118186354637146\n",
      "step: 253 loss: 0.0911497175693512\n",
      "step: 254 loss: 0.18688569962978363\n",
      "step: 255 loss: 0.18400244414806366\n",
      "step: 256 loss: 0.2608906626701355\n",
      "step: 257 loss: 0.2703910171985626\n",
      "step: 258 loss: 0.15117038786411285\n",
      "step: 259 loss: 0.1498032957315445\n",
      "step: 260 loss: 0.10782695561647415\n",
      "step: 261 loss: 0.10359767079353333\n",
      "step: 262 loss: 0.1629103124141693\n",
      "step: 263 loss: 0.16773076355457306\n",
      "step: 264 loss: 0.1782142072916031\n",
      "step: 265 loss: 0.12804734706878662\n",
      "step: 266 loss: 0.13755331933498383\n",
      "step: 267 loss: 0.16056041419506073\n",
      "step: 268 loss: 0.19681453704833984\n",
      "step: 269 loss: 0.26193711161613464\n",
      "step: 270 loss: 0.17333750426769257\n",
      "step: 271 loss: 0.07069739699363708\n",
      "step: 272 loss: 0.0963936448097229\n",
      "step: 273 loss: 0.12240801006555557\n",
      "step: 274 loss: 0.12047508358955383\n",
      "step: 275 loss: 0.11939702928066254\n",
      "step: 276 loss: 0.19623203575611115\n",
      "step: 277 loss: 0.10728244483470917\n",
      "step: 278 loss: 0.09539531916379929\n",
      "step: 279 loss: 0.08741170167922974\n",
      "step: 280 loss: 0.13839533925056458\n",
      "step: 281 loss: 0.08443580567836761\n",
      "step: 282 loss: 0.15710559487342834\n",
      "step: 283 loss: 0.2019692212343216\n",
      "step: 284 loss: 0.17839989066123962\n",
      "step: 285 loss: 0.0992620438337326\n",
      "step: 286 loss: 0.0822833999991417\n",
      "step: 287 loss: 0.1429668366909027\n",
      "step: 288 loss: 0.07925879955291748\n",
      "step: 289 loss: 0.08552482724189758\n",
      "step: 290 loss: 0.1029047891497612\n",
      "step: 291 loss: 0.1290610134601593\n",
      "step: 292 loss: 0.15409117937088013\n",
      "step: 293 loss: 0.15702848136425018\n",
      "step: 294 loss: 0.14570799469947815\n",
      "step: 295 loss: 0.08718200773000717\n",
      "step: 296 loss: 0.23793059587478638\n",
      "step: 297 loss: 0.2977049946784973\n",
      "step: 298 loss: 0.09162551909685135\n",
      "step: 299 loss: 0.18060897290706635\n",
      "step: 300 loss: 0.1728985607624054\n",
      "step: 301 loss: 0.10547631978988647\n",
      "step: 302 loss: 0.2528958022594452\n",
      "step: 303 loss: 0.16360920667648315\n",
      "step: 304 loss: 0.13979490101337433\n",
      "step: 305 loss: 0.026634521782398224\n",
      "step: 306 loss: 0.10017789155244827\n",
      "step: 307 loss: 0.0713992640376091\n",
      "step: 308 loss: 0.17073024809360504\n",
      "step: 309 loss: 0.06370298564434052\n",
      "step: 310 loss: 0.09562637656927109\n",
      "step: 311 loss: 0.16414731740951538\n",
      "step: 312 loss: 0.14332544803619385\n",
      "step: 313 loss: 0.08889711648225784\n",
      "step: 314 loss: 0.08184060454368591\n",
      "step: 315 loss: 0.07729773223400116\n",
      "step: 316 loss: 0.15386317670345306\n",
      "step: 317 loss: 0.08931034058332443\n",
      "step: 318 loss: 0.11050277948379517\n",
      "step: 319 loss: 0.07482551783323288\n",
      "step: 320 loss: 0.08363194763660431\n",
      "step: 321 loss: 0.09545479714870453\n",
      "step: 322 loss: 0.09771373867988586\n",
      "step: 323 loss: 0.0651065930724144\n",
      "step: 324 loss: 0.0818941593170166\n",
      "step: 325 loss: 0.2592746615409851\n",
      "step: 326 loss: 0.10636904835700989\n",
      "step: 327 loss: 0.04671871289610863\n",
      "step: 328 loss: 0.11762905865907669\n",
      "step: 329 loss: 0.0744468942284584\n",
      "step: 330 loss: 0.14879676699638367\n",
      "step: 331 loss: 0.0870421901345253\n",
      "step: 332 loss: 0.05842294543981552\n",
      "step: 333 loss: 0.0870295912027359\n",
      "step: 334 loss: 0.09380333125591278\n",
      "step: 335 loss: 0.2680591642856598\n",
      "step: 336 loss: 0.054329004138708115\n",
      "step: 337 loss: 0.12153275310993195\n",
      "step: 338 loss: 0.21439503133296967\n",
      "step: 339 loss: 0.1689772754907608\n",
      "step: 340 loss: 0.09571715444326401\n",
      "step: 341 loss: 0.07354530692100525\n",
      "step: 342 loss: 0.1745547354221344\n",
      "step: 343 loss: 0.06987155228853226\n",
      "step: 344 loss: 0.13200949132442474\n",
      "step: 345 loss: 0.11807137727737427\n",
      "step: 346 loss: 0.06481602042913437\n",
      "step: 347 loss: 0.11889177560806274\n",
      "step: 348 loss: 0.08770071715116501\n",
      "step: 349 loss: 0.16265860199928284\n",
      "step: 350 loss: 0.04140380397439003\n",
      "step: 351 loss: 0.08060281723737717\n",
      "step: 352 loss: 0.04562500864267349\n",
      "step: 353 loss: 0.07104957103729248\n",
      "step: 354 loss: 0.09038601070642471\n",
      "step: 355 loss: 0.18244829773902893\n",
      "step: 356 loss: 0.11229988932609558\n",
      "step: 357 loss: 0.3209000527858734\n",
      "step: 358 loss: 0.24859462678432465\n",
      "step: 359 loss: 0.046492740511894226\n",
      "step: 360 loss: 0.07138337194919586\n",
      "step: 361 loss: 0.1382182538509369\n",
      "step: 362 loss: 0.3210326135158539\n",
      "step: 363 loss: 0.11269541829824448\n",
      "step: 364 loss: 0.14219823479652405\n",
      "step: 365 loss: 0.29102906584739685\n",
      "step: 366 loss: 0.11480793356895447\n",
      "step: 367 loss: 0.12699130177497864\n",
      "step: 368 loss: 0.11318421363830566\n",
      "step: 369 loss: 0.04037965461611748\n",
      "step: 370 loss: 0.06929575651884079\n",
      "step: 371 loss: 0.054709866642951965\n",
      "step: 372 loss: 0.1252371072769165\n",
      "step: 373 loss: 0.1520184427499771\n",
      "step: 374 loss: 0.08973456174135208\n",
      "step: 375 loss: 0.05025981366634369\n",
      "step: 376 loss: 0.07345506548881531\n",
      "step: 377 loss: 0.1088336780667305\n",
      "step: 378 loss: 0.14487490057945251\n",
      "step: 379 loss: 0.26687389612197876\n",
      "step: 380 loss: 0.2045196145772934\n",
      "step: 381 loss: 0.10058274865150452\n",
      "step: 382 loss: 0.06412938237190247\n",
      "step: 383 loss: 0.030105698853731155\n",
      "step: 384 loss: 0.10643157362937927\n",
      "step: 385 loss: 0.1360897570848465\n",
      "step: 386 loss: 0.1686239391565323\n",
      "step: 387 loss: 0.062433503568172455\n",
      "step: 388 loss: 0.1520477533340454\n",
      "step: 389 loss: 0.10914648324251175\n",
      "step: 390 loss: 0.14587771892547607\n",
      "step: 391 loss: 0.07696506381034851\n",
      "step: 392 loss: 0.10373260825872421\n",
      "step: 393 loss: 0.14671242237091064\n",
      "step: 394 loss: 0.1885358840227127\n",
      "step: 395 loss: 0.10121358931064606\n",
      "step: 396 loss: 0.11687877774238586\n",
      "step: 397 loss: 0.07030495256185532\n",
      "step: 398 loss: 0.0563834086060524\n",
      "step: 399 loss: 0.07909417897462845\n",
      "step: 400 loss: 0.1867624819278717\n",
      "step: 401 loss: 0.16217423975467682\n",
      "step: 402 loss: 0.14642304182052612\n",
      "step: 403 loss: 0.24046829342842102\n",
      "step: 404 loss: 0.20556361973285675\n",
      "step: 405 loss: 0.1189080998301506\n",
      "step: 406 loss: 0.21838846802711487\n",
      "step: 407 loss: 0.17062759399414062\n",
      "step: 408 loss: 0.16469544172286987\n",
      "step: 409 loss: 0.07297758758068085\n",
      "step: 410 loss: 0.06495525687932968\n",
      "step: 411 loss: 0.06787166744470596\n",
      "step: 412 loss: 0.07852090150117874\n",
      "step: 413 loss: 0.13615575432777405\n",
      "step: 414 loss: 0.14646103978157043\n",
      "step: 415 loss: 0.12836432456970215\n",
      "step: 416 loss: 0.2964770793914795\n",
      "step: 417 loss: 0.1618088185787201\n",
      "step: 418 loss: 0.13035321235656738\n",
      "step: 419 loss: 0.060844194144010544\n",
      "step: 420 loss: 0.13923898339271545\n",
      "step: 421 loss: 0.03656652569770813\n",
      "step: 422 loss: 0.07645449042320251\n",
      "step: 423 loss: 0.14808359742164612\n",
      "step: 424 loss: 0.17281000316143036\n",
      "step: 425 loss: 0.07852794975042343\n",
      "step: 426 loss: 0.11911123991012573\n",
      "step: 427 loss: 0.0719294548034668\n",
      "step: 428 loss: 0.1239347830414772\n",
      "step: 429 loss: 0.10392413288354874\n",
      "step: 430 loss: 0.1577807366847992\n",
      "step: 431 loss: 0.05671627074480057\n",
      "step: 432 loss: 0.06111571937799454\n",
      "step: 433 loss: 0.1409129947423935\n",
      "step: 434 loss: 0.05754939094185829\n",
      "step: 435 loss: 0.12778957188129425\n",
      "step: 436 loss: 0.18807357549667358\n",
      "step: 437 loss: 0.06786923110485077\n",
      "step: 438 loss: 0.07917144149541855\n",
      "step: 439 loss: 0.10824108123779297\n",
      "step: 440 loss: 0.18404251337051392\n",
      "step: 441 loss: 0.09761907905340195\n",
      "step: 442 loss: 0.04715516045689583\n",
      "step: 443 loss: 0.0755692645907402\n",
      "step: 444 loss: 0.17642812430858612\n",
      "step: 445 loss: 0.11686022579669952\n",
      "step: 446 loss: 0.044219907373189926\n",
      "step: 447 loss: 0.16025108098983765\n",
      "step: 448 loss: 0.12082374840974808\n",
      "step: 449 loss: 0.0842018574476242\n",
      "step: 450 loss: 0.07004966586828232\n",
      "step: 451 loss: 0.06035060063004494\n",
      "step: 452 loss: 0.042613137513399124\n",
      "step: 453 loss: 0.034824974834918976\n",
      "step: 454 loss: 0.05048523098230362\n",
      "step: 455 loss: 0.030797038227319717\n",
      "step: 456 loss: 0.2083928883075714\n",
      "step: 457 loss: 0.15384148061275482\n",
      "step: 458 loss: 0.16913926601409912\n",
      "step: 459 loss: 0.04134627431631088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 460 loss: 0.13307665288448334\n",
      "step: 461 loss: 0.10938932746648788\n",
      "step: 462 loss: 0.03443401679396629\n",
      "step: 463 loss: 0.06547408550977707\n",
      "step: 464 loss: 0.03877031430602074\n",
      "step: 465 loss: 0.023705828934907913\n",
      "step: 466 loss: 0.10590185225009918\n",
      "step: 467 loss: 0.0684177428483963\n",
      "step: 468 loss: 0.07310835272073746\n",
      "step: 469 loss: 0.0306281466037035\n",
      "step: 470 loss: 0.21698857843875885\n",
      "step: 471 loss: 0.2126525491476059\n",
      "step: 472 loss: 0.292464017868042\n",
      "step: 473 loss: 0.12313944101333618\n",
      "step: 474 loss: 0.09717605262994766\n",
      "step: 475 loss: 0.07198337465524673\n",
      "step: 476 loss: 0.1158265620470047\n",
      "step: 477 loss: 0.08269722014665604\n",
      "step: 478 loss: 0.09573327004909515\n",
      "step: 479 loss: 0.08865087479352951\n",
      "step: 480 loss: 0.11767527461051941\n",
      "step: 481 loss: 0.04076649621129036\n",
      "step: 482 loss: 0.12104841321706772\n",
      "step: 483 loss: 0.12847387790679932\n",
      "step: 484 loss: 0.021216420456767082\n",
      "step: 485 loss: 0.07959320396184921\n",
      "step: 486 loss: 0.04996421933174133\n",
      "step: 487 loss: 0.02918371744453907\n",
      "step: 488 loss: 0.04026117920875549\n",
      "step: 489 loss: 0.1047460064291954\n",
      "step: 490 loss: 0.07070528715848923\n",
      "step: 491 loss: 0.09324418008327484\n",
      "step: 492 loss: 0.05701904743909836\n",
      "step: 493 loss: 0.07315481454133987\n",
      "step: 494 loss: 0.15697361528873444\n",
      "step: 495 loss: 0.20964986085891724\n",
      "step: 496 loss: 0.07216144353151321\n",
      "step: 497 loss: 0.08632612973451614\n",
      "step: 498 loss: 0.0675816610455513\n",
      "step: 499 loss: 0.14143739640712738\n",
      "step: 500 loss: 0.062167100608348846\n",
      "step: 501 loss: 0.15892159938812256\n",
      "step: 502 loss: 0.16065680980682373\n",
      "step: 503 loss: 0.03948419168591499\n",
      "step: 504 loss: 0.06593403965234756\n",
      "step: 505 loss: 0.1301436871290207\n",
      "step: 506 loss: 0.06983380019664764\n",
      "step: 507 loss: 0.07991251349449158\n",
      "step: 508 loss: 0.08310729265213013\n",
      "step: 509 loss: 0.1239604577422142\n",
      "step: 510 loss: 0.01173313520848751\n",
      "step: 511 loss: 0.10449056327342987\n",
      "step: 512 loss: 0.07245384901762009\n",
      "step: 513 loss: 0.051177978515625\n",
      "step: 514 loss: 0.0173186082392931\n",
      "step: 515 loss: 0.10116135329008102\n",
      "step: 516 loss: 0.07252946496009827\n",
      "step: 517 loss: 0.12987565994262695\n",
      "step: 518 loss: 0.05319932475686073\n",
      "step: 519 loss: 0.08087366819381714\n",
      "step: 520 loss: 0.10647065937519073\n",
      "step: 521 loss: 0.014698266983032227\n",
      "step: 522 loss: 0.01739928312599659\n",
      "step: 523 loss: 0.023891592398285866\n",
      "step: 524 loss: 0.09419127553701401\n",
      "step: 525 loss: 0.06928373128175735\n",
      "step: 526 loss: 0.018510686233639717\n",
      "step: 527 loss: 0.06487297266721725\n",
      "step: 528 loss: 0.09445926547050476\n",
      "step: 529 loss: 0.035590868443250656\n",
      "step: 530 loss: 0.08867025375366211\n",
      "step: 531 loss: 0.03845564275979996\n",
      "step: 532 loss: 0.05636082589626312\n",
      "step: 533 loss: 0.0844765305519104\n",
      "step: 534 loss: 0.01645052619278431\n",
      "step: 535 loss: 0.07535313814878464\n",
      "step: 536 loss: 0.08464295417070389\n",
      "step: 537 loss: 0.09774866700172424\n",
      "step: 538 loss: 0.12498273700475693\n",
      "step: 539 loss: 0.08834008127450943\n",
      "step: 540 loss: 0.06133507564663887\n",
      "step: 541 loss: 0.05405569449067116\n",
      "step: 542 loss: 0.03617498651146889\n",
      "step: 543 loss: 0.11758503317832947\n",
      "step: 544 loss: 0.037238460034132004\n",
      "step: 545 loss: 0.06475954502820969\n",
      "step: 546 loss: 0.014914211817085743\n",
      "step: 547 loss: 0.21856935322284698\n",
      "step: 548 loss: 0.09424033761024475\n",
      "step: 549 loss: 0.032769471406936646\n",
      "(10000, 784)\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 0.9900000095367432\n",
      "test accuracy = 0.949999988079071\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 0.9800000190734863\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 0.949999988079071\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 0.9800000190734863\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 0.9900000095367432\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 0.9599999785423279\n",
      "test accuracy = 0.9599999785423279\n",
      "test accuracy = 0.949999988079071\n",
      "test accuracy = 0.9800000190734863\n",
      "test accuracy = 0.9399999976158142\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 0.9599999785423279\n",
      "test accuracy = 0.949999988079071\n",
      "test accuracy = 1.0\n",
      "test accuracy = 0.9900000095367432\n",
      "test accuracy = 0.9800000190734863\n",
      "test accuracy = 0.9399999976158142\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 0.9900000095367432\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 1.0\n",
      "test accuracy = 0.949999988079071\n",
      "test accuracy = 1.0\n",
      "test accuracy = 0.9599999785423279\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 0.949999988079071\n",
      "test accuracy = 0.9800000190734863\n",
      "test accuracy = 0.9599999785423279\n",
      "test accuracy = 0.9599999785423279\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 0.9200000166893005\n",
      "test accuracy = 0.9599999785423279\n",
      "test accuracy = 0.9900000095367432\n",
      "test accuracy = 0.9900000095367432\n",
      "test accuracy = 0.9800000190734863\n",
      "test accuracy = 0.949999988079071\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 0.9800000190734863\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 0.9599999785423279\n",
      "test accuracy = 1.0\n",
      "test accuracy = 1.0\n",
      "test accuracy = 0.949999988079071\n",
      "test accuracy = 0.9800000190734863\n",
      "test accuracy = 0.9900000095367432\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 0.9800000190734863\n",
      "test accuracy = 0.949999988079071\n",
      "test accuracy = 1.0\n",
      "test accuracy = 0.9900000095367432\n",
      "test accuracy = 0.9900000095367432\n",
      "test accuracy = 0.9900000095367432\n",
      "test accuracy = 1.0\n",
      "test accuracy = 0.9800000190734863\n",
      "test accuracy = 0.9599999785423279\n",
      "test accuracy = 0.9900000095367432\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 0.9900000095367432\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 0.9800000190734863\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 1.0\n",
      "test accuracy = 0.9800000190734863\n",
      "test accuracy = 0.9800000190734863\n",
      "test accuracy = 1.0\n",
      "test accuracy = 0.9900000095367432\n",
      "test accuracy = 0.9900000095367432\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 0.9800000190734863\n",
      "test accuracy = 0.9900000095367432\n",
      "test accuracy = 0.9800000190734863\n",
      "test accuracy = 0.9900000095367432\n",
      "test accuracy = 1.0\n",
      "test accuracy = 0.9900000095367432\n",
      "test accuracy = 0.9599999785423279\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 1.0\n",
      "test accuracy = 1.0\n",
      "test accuracy = 0.9800000190734863\n",
      "test accuracy = 0.9800000190734863\n",
      "test accuracy = 0.9700000286102295\n",
      "test accuracy = 1.0\n",
      "test accuracy = 0.9900000095367432\n",
      "test accuracy = 0.9900000095367432\n",
      "test accuracy = 0.9900000095367432\n",
      "test accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import numpy as np\n",
    "\n",
    "tfe.enable_eager_execution()\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "class MNIST:\n",
    "    def __init__(self):\n",
    "        self.mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=True)\n",
    "        \n",
    "        self.train_ds = tf.data.Dataset.from_tensor_slices((self.mnist.train.images, self.mnist.train.labels))\\\n",
    "                        .map(lambda x, y: (x, tf.cast(y, tf.float32)))\\\n",
    "                        .shuffle(buffer_size=1000)\\\n",
    "                        .batch(100)\n",
    "        \n",
    "        self.test_ds = tf.data.Dataset.from_tensor_slices((self.mnist.test.images, self.mnist.test.labels))\\\n",
    "                        .map(lambda x, y: (x, tf.cast(y, tf.float32)))\\\n",
    "                        .shuffle(buffer_size=1000)\\\n",
    "                        .batch(100)\n",
    "        \n",
    "        self.filter1 = tf.get_variable(name=\"filter1\", shape=[5, 5, 1, 32], dtype=tf.float32)\n",
    "        self.bias1 = tf.get_variable(name=\"bias1\", shape=[32], dtype=tf.float32)\n",
    "              \n",
    "        self.filter2 = tf.get_variable(name=\"filter2\", shape=[5, 5, 32, 64], dtype=tf.float32)\n",
    "        self.bias2 = tf.get_variable(name=\"bias2\", shape=[64], dtype=tf.float32)\n",
    "\n",
    "        self.Weight_fc = tf.get_variable(name=\"full_connection_weight\", shape=[7 * 7 * 64, 1024], dtype=tf.float32)\n",
    "        self.Bias_fc = tf.get_variable(name=\"full_connection_bias\", shape=[1024], dtype=tf.float32)\n",
    " \n",
    "        self.Weight_sm = tf.get_variable(name=\"softmax_weight\", shape=[1024, 10], dtype=tf.float32)\n",
    "        self.Bias_sm = tf.get_variable(name=\"softmax_bias\", shape=[10], dtype=tf.float32)\n",
    "               \n",
    "    def cnn_model(self, image_batch):\n",
    "        # image_batch: 100 * 784\n",
    "        input_image = tf.reshape(image_batch, [-1, 28, 28, 1])\n",
    "        # construct first convolution layer\n",
    "        # step 1: initial filter\n",
    "        #input_image = tf.zeros([1, 28, 28, 1], dtype=tf.float32)\n",
    "        input_image = tf.to_float(input_image)\n",
    "        conv1_result = tf.nn.conv2d(input_image, self.filter1, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        relu_result = tf.nn.relu(conv1_result + self.bias1)\n",
    "        #print(relu_result.shape)\n",
    "        max_pool_result = tf.nn.max_pool(relu_result, [1, 2, 2, 1], [1, 2, 2, 1], padding=\"SAME\")\n",
    "        #print(max_pool_result.shape) # (1, 14, 14, 32)\n",
    "\n",
    "        # construct second convolution layer.\n",
    "        # step 2: second filter\n",
    "        conv2_result = tf.nn.conv2d(max_pool_result, self.filter2, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        relu2_result = tf.nn.relu(conv2_result + self.bias2)\n",
    "        #print(relu2_result.shape)\n",
    "        max_pool2_result = tf.nn.max_pool(relu2_result, [1, 2, 2, 1], [1, 2, 2, 1], padding=\"SAME\")\n",
    "        #print(max_pool2_result.shape)\n",
    "\n",
    "        # The difficulty is the dimension. Need to think carefully each time.\n",
    "\n",
    "        # full connection layer\n",
    "        full_connection_input = tf.reshape(max_pool2_result, [-1, 7 * 7 * 64])\n",
    "        full_connection_output = tf.nn.relu(tf.matmul(full_connection_input, self.Weight_fc) + self.Bias_fc)\n",
    "        #print(full_connection_output.shape)\n",
    "\n",
    "        # dropout and softmax \n",
    "        keep_prob = 0.5\n",
    "        softmax_input = tf.nn.dropout(full_connection_output, keep_prob)\n",
    "        y_conv = tf.nn.softmax(tf.matmul(softmax_input, self.Weight_sm) + self.Bias_sm)\n",
    "        #print(y_conv, y_conv.shape)\n",
    "\n",
    "        return y_conv\n",
    "        \n",
    "    def cross_entropy(self, image_batch, label_batch):\n",
    "        y = self.cnn_model(image_batch)\n",
    "        loss = tf.reduce_mean(-tf.reduce_sum(label_batch * tf.log(y), 1))\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def cal_gradient(self, image_batch, label_batch):\n",
    "        grad = tfe.implicit_value_and_gradients(self.cross_entropy)\n",
    "        \n",
    "        return grad(image_batch, label_batch)    \n",
    "    \n",
    "    def train(self):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "        #print(\"hello\")\n",
    "        for step, (image_batch, label_batch) in enumerate(tfe.Iterator(self.train_ds)):\n",
    "            loss, grads_and_vars = self.cal_gradient(image_batch, label_batch)\n",
    "            train_step = tf.train.GradientDescentOptimizer(0.5).apply_gradients(grads_and_vars) # learning rate is 0.5\n",
    "            print(\"step: {} loss: {}\".format(step, loss.numpy()))\n",
    "\n",
    "    def predict(self):\n",
    "        print(self.mnist.test.images.shape)\n",
    "        for step, (image_batch, label_batch) in enumerate(tfe.Iterator(self.test_ds)):\n",
    "            y = self.cnn_model(image_batch)\n",
    "            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(label_batch, 1)) # [true, true, false,..., true] boolen type\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))    # [1, 1, 0, ..., 1] with cast to convert\n",
    "            \n",
    "            print(\"test accuracy = {}\".format(accuracy.numpy()))\n",
    "           \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    mnist_model = MNIST()\n",
    "    mnist_model.train()\n",
    "    mnist_model.predict()\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
