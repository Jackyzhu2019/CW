{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment uses tensorflow eager excution mode to build the MNIST softmax regression model to do numerals recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let me give a simple example as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[12 32  4]], shape=(1, 3), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[1]\n",
      " [2]\n",
      " [3]], shape=(3, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[12 32  4]\n",
      " [24 64  8]\n",
      " [36 96 12]], shape=(3, 3), dtype=int32)\n",
      "[[12 32  4]] (1, 3)\n",
      "[[1]\n",
      " [2]\n",
      " [3]] (3, 1)\n",
      "[[88]] (1, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import numpy as np\n",
    "tfe.enable_eager_execution()\n",
    "\n",
    "a = tf.constant([[12, 32, 4]])\n",
    "b = tf.constant([[1], [2], [3]])\n",
    "\n",
    "c = tf.matmul(b, a)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "\n",
    "m = np.array([[12, 32, 4]])\n",
    "n = np.array([[1], [2], [3]])\n",
    "\n",
    "p = np.matmul(m, n)\n",
    "\n",
    "print(m, m.shape)\n",
    "print(n, n.shape)\n",
    "print(p, p.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we list 4 functions about calculate the gradients.\n",
    "f(x) = O(x, y, z, .... m)\n",
    "tfe.gradients_function()   # cal gradient for x\n",
    "\n",
    "tfe.value_and_gradients_function() # cal f(x) value and gradient for x\n",
    "\n",
    "tfe.implicit_gradients()  # cal gradient for y, z, ..., m\n",
    "\n",
    "tfe.implicit_value_and_gradients() # cal f(x) and gradient for y, z, ..., m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "#######\n",
      "9.0\n",
      "#######\n",
      "[(<tf.Tensor: id=2103, shape=(), dtype=float32, numpy=-24.0>, <tf.Variable 'x:0' shape=() dtype=float32, numpy=2.0>)]\n",
      "#######\n",
      "[<tf.Tensor: id=2139, shape=(), dtype=float32, numpy=6.0>]\n",
      "(<tf.Tensor: id=2186, shape=(), dtype=float32, numpy=9.0>, [<tf.Tensor: id=2198, shape=(), dtype=float32, numpy=6.0>])\n",
      "(<tf.Tensor: id=2245, shape=(), dtype=float32, numpy=9.0>, [(<tf.Tensor: id=2282, shape=(), dtype=float32, numpy=-24.0>, <tf.Variable 'x:0' shape=() dtype=float32, numpy=2.0>)])\n",
      "#######\n",
      "<tf.Variable 'x:0' shape=() dtype=float32, numpy=2.0>\n",
      "#######\n",
      "144.0\n",
      "#######\n",
      "[(<tf.Tensor: id=2406, shape=(), dtype=float32, numpy=-96.0>, <tf.Variable 'x:0' shape=() dtype=float32, numpy=2.0>), (<tf.Tensor: id=2381, shape=(), dtype=float32, numpy=144.0>, <tf.Variable 'z:0' shape=() dtype=float32, numpy=3.0>)]\n",
      "#######\n",
      "(<tf.Tensor: id=2436, shape=(), dtype=float32, numpy=144.0>, [(<tf.Tensor: id=2497, shape=(), dtype=float32, numpy=-96.0>, <tf.Variable 'x:0' shape=() dtype=float32, numpy=2.0>), (<tf.Tensor: id=2472, shape=(), dtype=float32, numpy=144.0>, <tf.Variable 'z:0' shape=() dtype=float32, numpy=3.0>)])\n",
      "#######\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import numpy as np\n",
    "tfe.enable_eager_execution()\n",
    "\n",
    "\n",
    "x = tfe.Variable(initial_value = 2.0, name = 'x')\n",
    "y = tfe.Variable(initial_value = 3.0, name = 'y')\n",
    "print(x.numpy())\n",
    "print(\"#######\")\n",
    "# 2\n",
    "def loss(y):\n",
    "    return (y - x ** 2) ** 2\n",
    "print(loss(7).numpy())\n",
    "print(\"#######\")\n",
    "# 9\n",
    "grad = tfe.implicit_gradients(loss)\n",
    "print(grad(7.))\n",
    "print(\"#######\")\n",
    "# cal gradient for xï¼š2 * (y - x ** 2)(-2 * x) = 2 * (7 - 2 ** 2) (-2 * 2) = 2 * 3 * (-4) = -24\n",
    "# [(<tf.Tensor: id=834, shape=(), dtype=float32, numpy=-24.0>, <tf.Variable 'x:0' shape=() dtype=float32, numpy=2.0>)]\n",
    "\n",
    "grad = tfe.gradients_function(loss)\n",
    "print(grad(7.0))\n",
    "# cal grad for y: 2 * (y - x^2) = 2 * (7-2^2) =6\n",
    "# [<tf.Tensor: id=2139, shape=(), dtype=float32, numpy=6.0>]\n",
    "\n",
    "grad = tfe.value_and_gradients_function(loss)\n",
    "print(grad(7.0))\n",
    "# cal value and grad for y\n",
    "# (<tf.Tensor: id=2186, shape=(), dtype=float32, numpy=9.0>, [<tf.Tensor: id=2198, shape=(), dtype=float32, numpy=6.0>])\n",
    "\n",
    "grad = tfe.implicit_value_and_gradients(loss)\n",
    "print(grad(7.))\n",
    "print(\"#######\")  \n",
    "# cal value and the grad for x: value = 9 and grad for x = -24\n",
    "# (<tf.Tensor: id=1220, shape=(), dtype=float32, numpy=9.0>, \n",
    "# [(<tf.Tensor: id=1257, shape=(), dtype=float32, numpy=-24.0>, <tf.Variable 'x:0' shape=() dtype=float32, numpy=2.0>)])\n",
    "\n",
    "x = tfe.Variable(initial_value = 2.0, name = 'x')\n",
    "y = tfe.Variable(initial_value = 3.0, name = 'y')\n",
    "z = tfe.Variable(initial_value = 3.0, name = 'z')\n",
    "print(x)\n",
    "print(\"#######\")\n",
    "# 2\n",
    "\n",
    "def loss(y):\n",
    "    return (y - x ** 2 + z ** 2) ** 2\n",
    "print(loss(7).numpy())\n",
    "print(\"#######\")\n",
    "# (7 - 2^2 + 3^2)^2 = 12 ^ 2 = 144\n",
    "grad = tfe.implicit_gradients(loss)\n",
    "print(grad(7.))\n",
    "print(\"#######\")\n",
    "# [(<tf.Tensor: id=1381, shape=(), dtype=float32, numpy=-96.0>, <tf.Variable 'x:0' shape=() dtype=float32, numpy=2.0>), \n",
    "# (<tf.Tensor: id=1356, shape=(), dtype=float32, numpy=144.0>, <tf.Variable 'z:0' shape=() dtype=float32, numpy=3.0>)]\n",
    "\n",
    "\n",
    "grad = tfe.implicit_value_and_gradients(loss)\n",
    "print(grad(7.))\n",
    "print(\"#######\")\n",
    "# (<tf.Tensor: id=1411, shape=(), dtype=float32, numpy=144.0>,\n",
    "# [(<tf.Tensor: id=1472, shape=(), dtype=float32, numpy=-96.0>, <tf.Variable 'x:0' shape=() dtype=float32, numpy=2.0>), \n",
    "# (<tf.Tensor: id=1447, shape=(), dtype=float32, numpy=144.0>, <tf.Variable 'z:0' shape=() dtype=float32, numpy=3.0>)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will show how to use Dataset and Iterators. \"https://www.tensorflow.org/api_docs/python/tf/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(11, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(13, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "(<tf.Tensor: id=545, shape=(), dtype=int32, numpy=30>, <tf.Tensor: id=546, shape=(), dtype=int32, numpy=60>)\n",
      "(<tf.Tensor: id=549, shape=(), dtype=int32, numpy=33>, <tf.Tensor: id=550, shape=(), dtype=int32, numpy=62>)\n",
      "(<tf.Tensor: id=553, shape=(), dtype=int32, numpy=36>, <tf.Tensor: id=554, shape=(), dtype=int32, numpy=64>)\n",
      "(<tf.Tensor: id=557, shape=(), dtype=int32, numpy=39>, <tf.Tensor: id=558, shape=(), dtype=int32, numpy=66>)\n",
      "(<tf.Tensor: id=561, shape=(), dtype=int32, numpy=42>, <tf.Tensor: id=562, shape=(), dtype=int32, numpy=68>)\n",
      "tf.Tensor([10 11 12 13 14], shape=(5,), dtype=int32)\n",
      "(<tf.Tensor: id=586, shape=(5,), dtype=int32, numpy=array([30, 33, 36, 39, 42])>, <tf.Tensor: id=587, shape=(5,), dtype=int32, numpy=array([60, 62, 64, 66, 68])>)\n",
      "tf.Tensor([0 1 2 3], shape=(4,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7], shape=(4,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import numpy as np\n",
    "tfe.enable_eager_execution()\n",
    "\n",
    "###### from_tensor_slices ######\n",
    "# Assume batch size is 1\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.range(10, 15))\n",
    "# Emits data of 10, 11, 12, 13, 14, (One element at a time)\n",
    "\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices((tf.range(30, 45, 3), np.arange(60, 70, 2)))\n",
    "# Emits data of (30, 60), (33, 62), (36, 64), (39, 66), (42, 68) one-to-one match\n",
    "# Emits one tuple at a time\n",
    "\n",
    "for i in tfe.Iterator(dataset1):\n",
    "    print(i)\n",
    "\n",
    "for i in tfe.Iterator(dataset2):\n",
    "    print(i)\n",
    "\n",
    "###### from_tensors ######\n",
    "dataset3 = tf.data.Dataset.from_tensors(tf.range(10, 15))\n",
    "# Emits data of [10, 11, 12, 13, 14]\n",
    "# Holds entire list as one element\n",
    "\n",
    "dataset4 = tf.data.Dataset.from_tensors((tf.range(30, 45, 3), np.arange(60, 70, 2)))\n",
    "# Emits data of ([30, 33, 36, 39, 42], [60, 62, 64, 66, 68])\n",
    "# Holds entire tuple as one element\n",
    "for i in tfe.Iterator(dataset3):\n",
    "    print(i)\n",
    "\n",
    "for i in tfe.Iterator(dataset4):\n",
    "    print(i)\n",
    "\n",
    "\n",
    "###### batch, repeat , shuffle ######\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "# Dataset: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "dataset1 = dataset1.batch(4)\n",
    "# Dataset: [0, 1, 2, 3], [4, 5, 6, 7], [8, 9]\n",
    "for i in tfe.Iterator(dataset1):\n",
    "    print(i)\n",
    "dataset1 = dataset1.repeat(2)\n",
    "# Dataset: [0, 1, 2, 3], [4, 5, 6, 7], [8, 9], [0, 1, 2, 3], [4, 5, 6, 7], [8, 9]\n",
    "# Notice a 2 element batch in between\n",
    "\n",
    "dataset1 = dataset1.shuffle(4)\n",
    "# Shuffles at batch level.\n",
    "# Dataset: [0, 1, 2, 3], [4, 5, 6, 7], [8, 9], [8, 9], [0, 1, 2, 3], [4, 5, 6, 7]\n",
    "\n",
    "\n",
    "# Ordering #2\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "# Dataset: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "dataset2 = dataset2.shuffle(4)\n",
    "# Dataset: [3, 1, 0, 4, 5, 8, 6, 9, 7, 2]\n",
    "\n",
    "dataset2 = dataset2.repeat(2)\n",
    "# Dataset: [3, 1, 0, 4, 5, 8, 6, 9, 7, 2, 3, 1, 0, 4, 5, 8, 6, 9, 7, 2]\n",
    "\n",
    "dataset2 = dataset2.batch(4)\n",
    "\n",
    "# Dataset: [3, 1, 0, 4], [5, 8, 6, 9], [7, 2, 3, 1], [0, 4, 5, 8], [6, 9, 7, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us use eager mode to build the MNIST softmax regression model to do numerals recognition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step: 0 loss: 2.518420934677124\n",
      "step: 1 loss: 1.9672133922576904\n",
      "step: 2 loss: 1.9115958213806152\n",
      "step: 3 loss: 1.6100528240203857\n",
      "step: 4 loss: 1.3711836338043213\n",
      "step: 5 loss: 1.555497169494629\n",
      "step: 6 loss: 1.5340849161148071\n",
      "step: 7 loss: 0.9834867715835571\n",
      "step: 8 loss: 0.8099104166030884\n",
      "step: 9 loss: 0.910269021987915\n",
      "step: 10 loss: 0.7270058989524841\n",
      "step: 11 loss: 0.9075319170951843\n",
      "step: 12 loss: 0.9778170585632324\n",
      "step: 13 loss: 0.9014764428138733\n",
      "step: 14 loss: 0.7988381385803223\n",
      "step: 15 loss: 0.6955233216285706\n",
      "step: 16 loss: 0.8558200001716614\n",
      "step: 17 loss: 0.6183838844299316\n",
      "step: 18 loss: 0.7889552116394043\n",
      "step: 19 loss: 0.6219127178192139\n",
      "step: 20 loss: 0.5840185880661011\n",
      "step: 21 loss: 0.7579265832901001\n",
      "step: 22 loss: 0.6852337121963501\n",
      "step: 23 loss: 0.6841601729393005\n",
      "step: 24 loss: 0.5916891694068909\n",
      "step: 25 loss: 0.6086613535881042\n",
      "step: 26 loss: 0.597419261932373\n",
      "step: 27 loss: 0.5059692859649658\n",
      "step: 28 loss: 0.7455536127090454\n",
      "step: 29 loss: 0.6554763913154602\n",
      "step: 30 loss: 0.5115291476249695\n",
      "step: 31 loss: 0.7174004316329956\n",
      "step: 32 loss: 0.4843039810657501\n",
      "step: 33 loss: 0.46546417474746704\n",
      "step: 34 loss: 0.46240007877349854\n",
      "step: 35 loss: 0.4839751124382019\n",
      "step: 36 loss: 0.631163477897644\n",
      "step: 37 loss: 0.6250154376029968\n",
      "step: 38 loss: 0.5117484331130981\n",
      "step: 39 loss: 0.5153052806854248\n",
      "step: 40 loss: 0.5709636211395264\n",
      "step: 41 loss: 0.4056776463985443\n",
      "step: 42 loss: 0.45021355152130127\n",
      "step: 43 loss: 0.5257731676101685\n",
      "step: 44 loss: 0.5383477210998535\n",
      "step: 45 loss: 0.4928058385848999\n",
      "step: 46 loss: 0.3536960184574127\n",
      "step: 47 loss: 0.38316819071769714\n",
      "step: 48 loss: 0.5512861609458923\n",
      "step: 49 loss: 0.460052490234375\n",
      "step: 50 loss: 0.5488415360450745\n",
      "step: 51 loss: 0.4852387607097626\n",
      "step: 52 loss: 0.32836097478866577\n",
      "step: 53 loss: 0.4931684732437134\n",
      "step: 54 loss: 0.2629479169845581\n",
      "step: 55 loss: 0.3754821717739105\n",
      "step: 56 loss: 0.46309012174606323\n",
      "step: 57 loss: 0.3912927508354187\n",
      "step: 58 loss: 0.47945794463157654\n",
      "step: 59 loss: 0.6324021816253662\n",
      "step: 60 loss: 0.6212419867515564\n",
      "step: 61 loss: 0.542402446269989\n",
      "step: 62 loss: 0.486294150352478\n",
      "step: 63 loss: 0.35308513045310974\n",
      "step: 64 loss: 0.3942812979221344\n",
      "step: 65 loss: 0.45315608382225037\n",
      "step: 66 loss: 0.4078425168991089\n",
      "step: 67 loss: 0.3297559320926666\n",
      "step: 68 loss: 0.49291104078292847\n",
      "step: 69 loss: 0.46148887276649475\n",
      "step: 70 loss: 0.5884891748428345\n",
      "step: 71 loss: 0.5124979615211487\n",
      "step: 72 loss: 0.5298706293106079\n",
      "step: 73 loss: 0.4798471927642822\n",
      "step: 74 loss: 0.45264968276023865\n",
      "step: 75 loss: 0.42874419689178467\n",
      "step: 76 loss: 0.4259951710700989\n",
      "step: 77 loss: 0.4903152585029602\n",
      "step: 78 loss: 0.43721580505371094\n",
      "step: 79 loss: 0.497671514749527\n",
      "step: 80 loss: 0.6429265737533569\n",
      "step: 81 loss: 0.5176233053207397\n",
      "step: 82 loss: 0.5129110813140869\n",
      "step: 83 loss: 0.4535115957260132\n",
      "step: 84 loss: 0.538770854473114\n",
      "step: 85 loss: 0.47792133688926697\n",
      "step: 86 loss: 0.6062108874320984\n",
      "step: 87 loss: 0.4157499670982361\n",
      "step: 88 loss: 0.5249304175376892\n",
      "step: 89 loss: 0.5639829635620117\n",
      "step: 90 loss: 0.4256199300289154\n",
      "step: 91 loss: 0.6683645844459534\n",
      "step: 92 loss: 0.6140191555023193\n",
      "step: 93 loss: 0.4077463150024414\n",
      "step: 94 loss: 0.32856932282447815\n",
      "step: 95 loss: 0.3696320652961731\n",
      "step: 96 loss: 0.45726826786994934\n",
      "step: 97 loss: 0.4025636315345764\n",
      "step: 98 loss: 0.42389219999313354\n",
      "step: 99 loss: 0.45202872157096863\n",
      "step: 100 loss: 0.5195609331130981\n",
      "step: 101 loss: 0.44839468598365784\n",
      "step: 102 loss: 0.35375693440437317\n",
      "step: 103 loss: 0.4289429187774658\n",
      "step: 104 loss: 0.5262362957000732\n",
      "step: 105 loss: 0.380350798368454\n",
      "step: 106 loss: 0.5080525875091553\n",
      "step: 107 loss: 0.3985058665275574\n",
      "step: 108 loss: 0.4649439752101898\n",
      "step: 109 loss: 0.4569398760795593\n",
      "step: 110 loss: 0.33703258633613586\n",
      "step: 111 loss: 0.5237978100776672\n",
      "step: 112 loss: 0.4296794831752777\n",
      "step: 113 loss: 0.5288604497909546\n",
      "step: 114 loss: 0.4957536458969116\n",
      "step: 115 loss: 0.46173104643821716\n",
      "step: 116 loss: 0.3747386932373047\n",
      "step: 117 loss: 0.47026604413986206\n",
      "step: 118 loss: 0.46509772539138794\n",
      "step: 119 loss: 0.5024936199188232\n",
      "step: 120 loss: 0.4158206582069397\n",
      "step: 121 loss: 0.5588316917419434\n",
      "step: 122 loss: 0.36259832978248596\n",
      "step: 123 loss: 0.39072418212890625\n",
      "step: 124 loss: 0.24252671003341675\n",
      "step: 125 loss: 0.45801690220832825\n",
      "step: 126 loss: 0.3459812104701996\n",
      "step: 127 loss: 0.3467000126838684\n",
      "step: 128 loss: 0.6013611555099487\n",
      "step: 129 loss: 0.34541621804237366\n",
      "step: 130 loss: 0.4073599874973297\n",
      "step: 131 loss: 0.33906394243240356\n",
      "step: 132 loss: 0.45983588695526123\n",
      "step: 133 loss: 0.32983720302581787\n",
      "step: 134 loss: 0.38386210799217224\n",
      "step: 135 loss: 0.22163738310337067\n",
      "step: 136 loss: 0.33148810267448425\n",
      "step: 137 loss: 0.3286854922771454\n",
      "step: 138 loss: 0.3758389353752136\n",
      "step: 139 loss: 0.16357210278511047\n",
      "step: 140 loss: 0.28508979082107544\n",
      "step: 141 loss: 0.5690665245056152\n",
      "step: 142 loss: 0.3243354856967926\n",
      "step: 143 loss: 0.5079834461212158\n",
      "step: 144 loss: 0.40937110781669617\n",
      "step: 145 loss: 0.5600040555000305\n",
      "step: 146 loss: 0.28483325242996216\n",
      "step: 147 loss: 0.5716725587844849\n",
      "step: 148 loss: 0.3020644783973694\n",
      "step: 149 loss: 0.32826709747314453\n",
      "step: 150 loss: 0.36854538321495056\n",
      "step: 151 loss: 0.2594429850578308\n",
      "step: 152 loss: 0.5275459289550781\n",
      "step: 153 loss: 0.1683436632156372\n",
      "step: 154 loss: 0.35123148560523987\n",
      "step: 155 loss: 0.2801150381565094\n",
      "step: 156 loss: 0.4109480381011963\n",
      "step: 157 loss: 0.3872307240962982\n",
      "step: 158 loss: 0.39036285877227783\n",
      "step: 159 loss: 0.40368324518203735\n",
      "step: 160 loss: 0.3466331362724304\n",
      "step: 161 loss: 0.3651143014431\n",
      "step: 162 loss: 0.266181081533432\n",
      "step: 163 loss: 0.31085583567619324\n",
      "step: 164 loss: 0.3429129719734192\n",
      "step: 165 loss: 0.22799800336360931\n",
      "step: 166 loss: 0.43954429030418396\n",
      "step: 167 loss: 0.4074002802371979\n",
      "step: 168 loss: 0.45056599378585815\n",
      "step: 169 loss: 0.3981681168079376\n",
      "step: 170 loss: 0.5961151719093323\n",
      "step: 171 loss: 0.31522828340530396\n",
      "step: 172 loss: 0.25432631373405457\n",
      "step: 173 loss: 0.18011046946048737\n",
      "step: 174 loss: 0.35807836055755615\n",
      "step: 175 loss: 0.4355281889438629\n",
      "step: 176 loss: 0.2515517771244049\n",
      "step: 177 loss: 0.43571585416793823\n",
      "step: 178 loss: 0.3809032440185547\n",
      "step: 179 loss: 0.2902807891368866\n",
      "step: 180 loss: 0.4809248745441437\n",
      "step: 181 loss: 0.4270632565021515\n",
      "step: 182 loss: 0.3823845684528351\n",
      "step: 183 loss: 0.3776702582836151\n",
      "step: 184 loss: 0.3728635311126709\n",
      "step: 185 loss: 0.328766793012619\n",
      "step: 186 loss: 0.3789221942424774\n",
      "step: 187 loss: 0.42535457015037537\n",
      "step: 188 loss: 0.35955604910850525\n",
      "step: 189 loss: 0.29054272174835205\n",
      "step: 190 loss: 0.33623871207237244\n",
      "step: 191 loss: 0.41133835911750793\n",
      "step: 192 loss: 0.3276076018810272\n",
      "step: 193 loss: 0.4517747163772583\n",
      "step: 194 loss: 0.5252776145935059\n",
      "step: 195 loss: 0.31410086154937744\n",
      "step: 196 loss: 0.27666395902633667\n",
      "step: 197 loss: 0.3059748411178589\n",
      "step: 198 loss: 0.38298091292381287\n",
      "step: 199 loss: 0.37935927510261536\n",
      "step: 200 loss: 0.3886031210422516\n",
      "step: 201 loss: 0.24559059739112854\n",
      "step: 202 loss: 0.29984980821609497\n",
      "step: 203 loss: 0.31810909509658813\n",
      "step: 204 loss: 0.358513742685318\n",
      "step: 205 loss: 0.3329414427280426\n",
      "step: 206 loss: 0.20490863919258118\n",
      "step: 207 loss: 0.49879464507102966\n",
      "step: 208 loss: 0.4054274260997772\n",
      "step: 209 loss: 0.33282896876335144\n",
      "step: 210 loss: 0.33384931087493896\n",
      "step: 211 loss: 0.45089226961135864\n",
      "step: 212 loss: 0.39362359046936035\n",
      "step: 213 loss: 0.30207696557044983\n",
      "step: 214 loss: 0.3956499397754669\n",
      "step: 215 loss: 0.32570987939834595\n",
      "step: 216 loss: 0.293178528547287\n",
      "step: 217 loss: 0.38031044602394104\n",
      "step: 218 loss: 0.39207732677459717\n",
      "step: 219 loss: 0.3039691150188446\n",
      "step: 220 loss: 0.3024352490901947\n",
      "step: 221 loss: 0.3077455759048462\n",
      "step: 222 loss: 0.4047328233718872\n",
      "step: 223 loss: 0.3956012427806854\n",
      "step: 224 loss: 0.4785182476043701\n",
      "step: 225 loss: 0.25881192088127136\n",
      "step: 226 loss: 0.45965495705604553\n",
      "step: 227 loss: 0.318261981010437\n",
      "step: 228 loss: 0.42146942019462585\n",
      "step: 229 loss: 0.44573402404785156\n",
      "step: 230 loss: 0.36934375762939453\n",
      "step: 231 loss: 0.15617051720619202\n",
      "step: 232 loss: 0.36391356587409973\n",
      "step: 233 loss: 0.41972190141677856\n",
      "step: 234 loss: 0.37267348170280457\n",
      "step: 235 loss: 0.28050103783607483\n",
      "step: 236 loss: 0.5511018633842468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 237 loss: 0.450685054063797\n",
      "step: 238 loss: 0.3476310074329376\n",
      "step: 239 loss: 0.3358263075351715\n",
      "step: 240 loss: 0.3243204355239868\n",
      "step: 241 loss: 0.4013921320438385\n",
      "step: 242 loss: 0.39357316493988037\n",
      "step: 243 loss: 0.41609418392181396\n",
      "step: 244 loss: 0.34057196974754333\n",
      "step: 245 loss: 0.454276442527771\n",
      "step: 246 loss: 0.4364652633666992\n",
      "step: 247 loss: 0.46663933992385864\n",
      "step: 248 loss: 0.310959130525589\n",
      "step: 249 loss: 0.460178017616272\n",
      "step: 250 loss: 0.2628069519996643\n",
      "step: 251 loss: 0.32114288210868835\n",
      "step: 252 loss: 0.4539031684398651\n",
      "step: 253 loss: 0.5069394707679749\n",
      "step: 254 loss: 0.3205595314502716\n",
      "step: 255 loss: 0.5110148191452026\n",
      "step: 256 loss: 0.5103697180747986\n",
      "step: 257 loss: 0.612093448638916\n",
      "step: 258 loss: 0.4614759087562561\n",
      "step: 259 loss: 0.3232191503047943\n",
      "step: 260 loss: 0.34048226475715637\n",
      "step: 261 loss: 0.414230078458786\n",
      "step: 262 loss: 0.3251868188381195\n",
      "step: 263 loss: 0.3347572386264801\n",
      "step: 264 loss: 0.41020721197128296\n",
      "step: 265 loss: 0.5440208315849304\n",
      "step: 266 loss: 0.3975584805011749\n",
      "step: 267 loss: 0.31369996070861816\n",
      "step: 268 loss: 0.5674089789390564\n",
      "step: 269 loss: 0.5456468462944031\n",
      "step: 270 loss: 0.5267543792724609\n",
      "step: 271 loss: 0.3649550974369049\n",
      "step: 272 loss: 0.49707382917404175\n",
      "step: 273 loss: 0.43261927366256714\n",
      "step: 274 loss: 0.5105885863304138\n",
      "step: 275 loss: 0.29197874665260315\n",
      "step: 276 loss: 0.45322734117507935\n",
      "step: 277 loss: 0.2994551956653595\n",
      "step: 278 loss: 0.33842381834983826\n",
      "step: 279 loss: 0.219589501619339\n",
      "step: 280 loss: 0.329645574092865\n",
      "step: 281 loss: 0.3299102783203125\n",
      "step: 282 loss: 0.18876540660858154\n",
      "step: 283 loss: 0.2442081868648529\n",
      "step: 284 loss: 0.22881996631622314\n",
      "step: 285 loss: 0.24205650389194489\n",
      "step: 286 loss: 0.2525228261947632\n",
      "step: 287 loss: 0.36352089047431946\n",
      "step: 288 loss: 0.4836359918117523\n",
      "step: 289 loss: 0.4507734775543213\n",
      "step: 290 loss: 0.2983284294605255\n",
      "step: 291 loss: 0.3558087944984436\n",
      "step: 292 loss: 0.2312258780002594\n",
      "step: 293 loss: 0.26065608859062195\n",
      "step: 294 loss: 0.3752576410770416\n",
      "step: 295 loss: 0.29694584012031555\n",
      "step: 296 loss: 0.43511053919792175\n",
      "step: 297 loss: 0.2666553258895874\n",
      "step: 298 loss: 0.42942532896995544\n",
      "step: 299 loss: 0.3553515672683716\n",
      "step: 300 loss: 0.2508274018764496\n",
      "step: 301 loss: 0.3293495178222656\n",
      "step: 302 loss: 0.36047857999801636\n",
      "step: 303 loss: 0.3755855858325958\n",
      "step: 304 loss: 0.2634878158569336\n",
      "step: 305 loss: 0.3690834939479828\n",
      "step: 306 loss: 0.3774018883705139\n",
      "step: 307 loss: 0.4308106601238251\n",
      "step: 308 loss: 0.1453976035118103\n",
      "step: 309 loss: 0.3866311013698578\n",
      "step: 310 loss: 0.24244533479213715\n",
      "step: 311 loss: 0.30480748414993286\n",
      "step: 312 loss: 0.3458268344402313\n",
      "step: 313 loss: 0.4083114564418793\n",
      "step: 314 loss: 0.2944912016391754\n",
      "step: 315 loss: 0.5087563395500183\n",
      "step: 316 loss: 0.41081222891807556\n",
      "step: 317 loss: 0.5068943500518799\n",
      "step: 318 loss: 0.688215970993042\n",
      "step: 319 loss: 0.16326487064361572\n",
      "step: 320 loss: 0.3798255920410156\n",
      "step: 321 loss: 0.3665851652622223\n",
      "step: 322 loss: 0.3044148087501526\n",
      "step: 323 loss: 0.2848530113697052\n",
      "step: 324 loss: 0.24979953467845917\n",
      "step: 325 loss: 0.30238574743270874\n",
      "step: 326 loss: 0.29109591245651245\n",
      "step: 327 loss: 0.3582206666469574\n",
      "step: 328 loss: 0.38038280606269836\n",
      "step: 329 loss: 0.38811346888542175\n",
      "step: 330 loss: 0.25892117619514465\n",
      "step: 331 loss: 0.3368717133998871\n",
      "step: 332 loss: 0.4786837697029114\n",
      "step: 333 loss: 0.2741996943950653\n",
      "step: 334 loss: 0.4460597336292267\n",
      "step: 335 loss: 0.47155702114105225\n",
      "step: 336 loss: 0.23952241241931915\n",
      "step: 337 loss: 0.28942131996154785\n",
      "step: 338 loss: 0.2536071538925171\n",
      "step: 339 loss: 0.3823351562023163\n",
      "step: 340 loss: 0.3330807387828827\n",
      "step: 341 loss: 0.3593892753124237\n",
      "step: 342 loss: 0.3939892649650574\n",
      "step: 343 loss: 0.4482978582382202\n",
      "step: 344 loss: 0.3825148344039917\n",
      "step: 345 loss: 0.21679891645908356\n",
      "step: 346 loss: 0.3701452612876892\n",
      "step: 347 loss: 0.19053635001182556\n",
      "step: 348 loss: 0.19904625415802002\n",
      "step: 349 loss: 0.2733384370803833\n",
      "step: 350 loss: 0.27182209491729736\n",
      "step: 351 loss: 0.4304087162017822\n",
      "step: 352 loss: 0.29216861724853516\n",
      "step: 353 loss: 0.5912433862686157\n",
      "step: 354 loss: 0.41474711894989014\n",
      "step: 355 loss: 0.2807637155056\n",
      "step: 356 loss: 0.39089852571487427\n",
      "step: 357 loss: 0.4820472002029419\n",
      "step: 358 loss: 0.3767487704753876\n",
      "step: 359 loss: 0.3502260446548462\n",
      "step: 360 loss: 0.23792850971221924\n",
      "step: 361 loss: 0.4745461940765381\n",
      "step: 362 loss: 0.40876907110214233\n",
      "step: 363 loss: 0.19957011938095093\n",
      "step: 364 loss: 0.3744297921657562\n",
      "step: 365 loss: 0.3242318630218506\n",
      "step: 366 loss: 0.4554244875907898\n",
      "step: 367 loss: 0.2909073531627655\n",
      "step: 368 loss: 0.27000120282173157\n",
      "step: 369 loss: 0.17535625398159027\n",
      "step: 370 loss: 0.49148276448249817\n",
      "step: 371 loss: 0.4210682809352875\n",
      "step: 372 loss: 0.34047386050224304\n",
      "step: 373 loss: 0.5504072308540344\n",
      "step: 374 loss: 0.40589621663093567\n",
      "step: 375 loss: 0.28308817744255066\n",
      "step: 376 loss: 0.41688305139541626\n",
      "step: 377 loss: 0.31494519114494324\n",
      "step: 378 loss: 0.26367852091789246\n",
      "step: 379 loss: 0.29118314385414124\n",
      "step: 380 loss: 0.19051696360111237\n",
      "step: 381 loss: 0.29219695925712585\n",
      "step: 382 loss: 0.4520857334136963\n",
      "step: 383 loss: 0.41681575775146484\n",
      "step: 384 loss: 0.3370020389556885\n",
      "step: 385 loss: 0.4338321387767792\n",
      "step: 386 loss: 0.2939571738243103\n",
      "step: 387 loss: 0.27562063932418823\n",
      "step: 388 loss: 0.4365442991256714\n",
      "step: 389 loss: 0.3872673809528351\n",
      "step: 390 loss: 0.22528205811977386\n",
      "step: 391 loss: 0.32946541905403137\n",
      "step: 392 loss: 0.49472513794898987\n",
      "step: 393 loss: 0.4271117150783539\n",
      "step: 394 loss: 0.460763543844223\n",
      "step: 395 loss: 0.41574057936668396\n",
      "step: 396 loss: 0.41467803716659546\n",
      "step: 397 loss: 0.46841728687286377\n",
      "step: 398 loss: 0.4169943928718567\n",
      "step: 399 loss: 0.3485085964202881\n",
      "step: 400 loss: 0.315056711435318\n",
      "step: 401 loss: 0.29175782203674316\n",
      "step: 402 loss: 0.34997397661209106\n",
      "step: 403 loss: 0.24408145248889923\n",
      "step: 404 loss: 0.446122944355011\n",
      "step: 405 loss: 0.38829001784324646\n",
      "step: 406 loss: 0.332753449678421\n",
      "step: 407 loss: 0.29630744457244873\n",
      "step: 408 loss: 0.25720345973968506\n",
      "step: 409 loss: 0.307399183511734\n",
      "step: 410 loss: 0.232689768075943\n",
      "step: 411 loss: 0.22456149756908417\n",
      "step: 412 loss: 0.2573351562023163\n",
      "step: 413 loss: 0.5956811904907227\n",
      "step: 414 loss: 0.33976349234580994\n",
      "step: 415 loss: 0.3661899268627167\n",
      "step: 416 loss: 0.281671404838562\n",
      "step: 417 loss: 0.43038222193717957\n",
      "step: 418 loss: 0.46040207147598267\n",
      "step: 419 loss: 0.25193652510643005\n",
      "step: 420 loss: 0.4099896550178528\n",
      "step: 421 loss: 0.2085656225681305\n",
      "step: 422 loss: 0.4936373233795166\n",
      "step: 423 loss: 0.39767077565193176\n",
      "step: 424 loss: 0.3147679567337036\n",
      "step: 425 loss: 0.228205606341362\n",
      "step: 426 loss: 0.24858646094799042\n",
      "step: 427 loss: 0.3139067590236664\n",
      "step: 428 loss: 0.3639267683029175\n",
      "step: 429 loss: 0.3399142324924469\n",
      "step: 430 loss: 0.37888777256011963\n",
      "step: 431 loss: 0.38683703541755676\n",
      "step: 432 loss: 0.22728987038135529\n",
      "step: 433 loss: 0.2822325825691223\n",
      "step: 434 loss: 0.3075885474681854\n",
      "step: 435 loss: 0.377564013004303\n",
      "step: 436 loss: 0.5069092512130737\n",
      "step: 437 loss: 0.3656557500362396\n",
      "step: 438 loss: 0.41093337535858154\n",
      "step: 439 loss: 0.55290687084198\n",
      "step: 440 loss: 0.36466163396835327\n",
      "step: 441 loss: 0.3049063980579376\n",
      "step: 442 loss: 0.38662615418434143\n",
      "step: 443 loss: 0.4028257131576538\n",
      "step: 444 loss: 0.42576950788497925\n",
      "step: 445 loss: 0.49948859214782715\n",
      "step: 446 loss: 0.51399165391922\n",
      "step: 447 loss: 0.4046812355518341\n",
      "step: 448 loss: 0.39858725666999817\n",
      "step: 449 loss: 0.1928233653306961\n",
      "step: 450 loss: 0.43199509382247925\n",
      "step: 451 loss: 0.326844185590744\n",
      "step: 452 loss: 0.3025255501270294\n",
      "step: 453 loss: 0.300862193107605\n",
      "step: 454 loss: 0.302015483379364\n",
      "step: 455 loss: 0.3283298909664154\n",
      "step: 456 loss: 0.20961818099021912\n",
      "step: 457 loss: 0.3319089114665985\n",
      "step: 458 loss: 0.19600540399551392\n",
      "step: 459 loss: 0.27887439727783203\n",
      "step: 460 loss: 0.29877641797065735\n",
      "step: 461 loss: 0.3484017550945282\n",
      "step: 462 loss: 0.24448876082897186\n",
      "step: 463 loss: 0.3066363036632538\n",
      "step: 464 loss: 0.39459237456321716\n",
      "step: 465 loss: 0.4590871334075928\n",
      "step: 466 loss: 0.17853881418704987\n",
      "step: 467 loss: 0.21765774488449097\n",
      "step: 468 loss: 0.4402047395706177\n",
      "step: 469 loss: 0.6009464859962463\n",
      "step: 470 loss: 0.5720022916793823\n",
      "step: 471 loss: 0.6554905772209167\n",
      "step: 472 loss: 0.34095442295074463\n",
      "step: 473 loss: 0.4926898181438446\n",
      "step: 474 loss: 0.313968688249588\n",
      "step: 475 loss: 0.27401241660118103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 476 loss: 0.4645126760005951\n",
      "step: 477 loss: 0.3733900487422943\n",
      "step: 478 loss: 0.3258801996707916\n",
      "step: 479 loss: 0.4040234386920929\n",
      "step: 480 loss: 0.4823179543018341\n",
      "step: 481 loss: 0.32798314094543457\n",
      "step: 482 loss: 0.25747188925743103\n",
      "step: 483 loss: 0.31094101071357727\n",
      "step: 484 loss: 0.2920394837856293\n",
      "step: 485 loss: 0.2934306263923645\n",
      "step: 486 loss: 0.3596334159374237\n",
      "step: 487 loss: 0.20532439649105072\n",
      "step: 488 loss: 0.34153884649276733\n",
      "step: 489 loss: 0.3228786885738373\n",
      "step: 490 loss: 0.4614839255809784\n",
      "step: 491 loss: 0.32868796586990356\n",
      "step: 492 loss: 0.4221988618373871\n",
      "step: 493 loss: 0.22081485390663147\n",
      "step: 494 loss: 0.3258853852748871\n",
      "step: 495 loss: 0.2938960790634155\n",
      "step: 496 loss: 0.26152297854423523\n",
      "step: 497 loss: 0.3245876431465149\n",
      "step: 498 loss: 0.3915487229824066\n",
      "step: 499 loss: 0.22904826700687408\n",
      "step: 500 loss: 0.20780354738235474\n",
      "step: 501 loss: 0.14822646975517273\n",
      "step: 502 loss: 0.2812418043613434\n",
      "step: 503 loss: 0.3145609200000763\n",
      "step: 504 loss: 0.21812568604946136\n",
      "step: 505 loss: 0.3359892964363098\n",
      "step: 506 loss: 0.27689942717552185\n",
      "step: 507 loss: 0.22364474833011627\n",
      "step: 508 loss: 0.3983500301837921\n",
      "step: 509 loss: 0.23038695752620697\n",
      "step: 510 loss: 0.2528464198112488\n",
      "step: 511 loss: 0.1640140563249588\n",
      "step: 512 loss: 0.5465072989463806\n",
      "step: 513 loss: 0.3377767503261566\n",
      "step: 514 loss: 0.2220756858587265\n",
      "step: 515 loss: 0.241927832365036\n",
      "step: 516 loss: 0.3196342885494232\n",
      "step: 517 loss: 0.22323016822338104\n",
      "step: 518 loss: 0.23621565103530884\n",
      "step: 519 loss: 0.3680189549922943\n",
      "step: 520 loss: 0.19240455329418182\n",
      "step: 521 loss: 0.23800715804100037\n",
      "step: 522 loss: 0.15422265231609344\n",
      "step: 523 loss: 0.17983315885066986\n",
      "step: 524 loss: 0.202163428068161\n",
      "step: 525 loss: 0.332660436630249\n",
      "step: 526 loss: 0.2807442545890808\n",
      "step: 527 loss: 0.18434982001781464\n",
      "step: 528 loss: 0.2773273289203644\n",
      "step: 529 loss: 0.23401103913784027\n",
      "step: 530 loss: 0.28774526715278625\n",
      "step: 531 loss: 0.1785891354084015\n",
      "step: 532 loss: 0.1016918197274208\n",
      "step: 533 loss: 0.3391100764274597\n",
      "step: 534 loss: 0.24107162654399872\n",
      "step: 535 loss: 0.1904778629541397\n",
      "step: 536 loss: 0.27750709652900696\n",
      "step: 537 loss: 0.17841221392154694\n",
      "step: 538 loss: 0.22300636768341064\n",
      "step: 539 loss: 0.22038140892982483\n",
      "step: 540 loss: 0.1668100506067276\n",
      "step: 541 loss: 0.23177212476730347\n",
      "step: 542 loss: 0.15435250103473663\n",
      "step: 543 loss: 0.2197788655757904\n",
      "step: 544 loss: 0.3304308354854584\n",
      "step: 545 loss: 0.24846184253692627\n",
      "step: 546 loss: 0.3063674867153168\n",
      "step: 547 loss: 0.15653090178966522\n",
      "step: 548 loss: 0.3571031093597412\n",
      "step: 549 loss: 0.36542809009552\n",
      "test accuracy = 0.9117000102996826\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tfe.enable_eager_execution()\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "class MNIST:\n",
    "    def __init__(self):\n",
    "        self.mnist = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)\n",
    "        self.W = tf.get_variable(name=\"W\", shape=(784, 10)) # use tf.get_variable to replace tf.Variable\n",
    "        self.b = tf.get_variable(name=\"b\", shape=(10))\n",
    "\n",
    "        self.train_ds = tf.data.Dataset.from_tensor_slices((self.mnist.train.images, self.mnist.train.labels))\\\n",
    "                                       .map(lambda x, y: (x, tf.cast(y, tf.float32)))\\\n",
    "                                       .shuffle(buffer_size = 1000)\\\n",
    "                                       .batch(100)\n",
    "\n",
    "    def softmax_model(self, image_batch):\n",
    "        y = tf.nn.softmax(tf.matmul(image_batch, self.W) + self.b)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def cross_entropy(self, image_batch, label_batch):\n",
    "        y = self.softmax_model(image_batch)\n",
    "        loss = tf.reduce_mean(-tf.reduce_sum(label_batch * tf.log(y), 1))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def cal_gradient(self, image_batch, label_batch):\n",
    "        grad = tfe.implicit_value_and_gradients(self.cross_entropy)\n",
    "        # A logic here:\n",
    "        # we should 1) construct a function like y = x^2\n",
    "        # 2) then we calculate gradients function, i.e., y = 2x\n",
    "        # 3) finally we apply x = 4 to gradients function y = 2x\n",
    "        # In this case, we cannot use cross_entropy(image_batch, label_batch) directly. \n",
    "        \n",
    "        return grad(image_batch, label_batch)\n",
    "    \n",
    "    def train(self):   \n",
    "        for step, (image_batch, label_batch) in enumerate(tfe.Iterator(self.train_ds)):\n",
    "        #    print(image_batch.shape)\n",
    "            \n",
    "            loss, grads_and_vars = self.cal_gradient(image_batch, label_batch)\n",
    "            train_step = tf.train.GradientDescentOptimizer(0.5).apply_gradients(grads_and_vars) # learning rate is 0.5\n",
    "            # Why not diretly use minimzie() as follows:\n",
    "            #loss = self.cross_entropy(image_batch, label_batch)\n",
    "            #train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "            # if like this, loss will be a real value, not gradient function.\n",
    "            print(\"step: {} loss: {}\".format(step, loss.numpy()))\n",
    "\n",
    "    def predict(self):\n",
    "        y = self.softmax_model(self.mnist.test.images)\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(self.mnist.test.labels, 1)) # [true, true, false,..., true] boolen type\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))    # [1, 1, 0, ..., 1] with cast to convert\n",
    "\n",
    "        print(\"test accuracy = {}\".format(accuracy.numpy()))\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    mnist_model = MNIST()\n",
    "    mnist_model.train()\n",
    "    mnist_model.predict()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
