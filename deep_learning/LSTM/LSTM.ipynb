{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This page will give some descriptions about LSTM. This page will give some descriptions about LSTM(Long short term memory). Its explanation will be obtained from http://colah.github.io/posts/2015-08-Understanding-LSTMs/. Compared with RNN, the major difference is the design in hidden layer. RNN has a simple hidden layer design, simply using previous state, current input to multiply hidden neurons, and then activate it with activation function. However, with LSTM, it has a more complexed design, with many gates, such as forget gate, input gate, candidate gate and output gate, which will help reduce the vanishing gradient problem, where we can have more layers and more inputs(i.e. more historical info).\n",
    "Next I will use the tensorflow to implement LSTM for prediction in Penn Tree Bank(PTB) dataset. The program is modified from tensorflow official website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0818 21:48:29.086984  5472 deprecation_wrapper.py:119] From D:\\ai_lab\\LSTM\\reader.py:31: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0818 21:48:29.748011  5472 deprecation.py:323] From <ipython-input-1-b7f4f743e503>:31: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0818 21:48:32.000098  5472 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0818 21:48:32.031680  5472 deprecation.py:323] From <ipython-input-1-b7f4f743e503>:37: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "W0818 21:48:32.201848  5472 deprecation.py:506] From <ipython-input-1-b7f4f743e503>:47: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0818 21:48:32.404031  5472 deprecation.py:506] From d:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0818 21:48:35.345330  5472 deprecation.py:323] From d:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0818 21:48:44.707712  5472 deprecation_wrapper.py:119] From D:\\ai_lab\\LSTM\\reader.py:114: The name tf.assert_positive is deprecated. Please use tf.compat.v1.assert_positive instead.\n",
      "\n",
      "W0818 21:48:44.725499  5472 deprecation.py:323] From D:\\ai_lab\\LSTM\\reader.py:120: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "W0818 21:48:44.733805  5472 deprecation.py:323] From d:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\input.py:320: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "W0818 21:48:44.737513  5472 deprecation.py:323] From d:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "W0818 21:48:44.739271  5472 deprecation.py:323] From d:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "W0818 21:48:44.739997  5472 deprecation.py:323] From d:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "W0818 21:48:45.072983  5472 deprecation.py:323] From <ipython-input-1-b7f4f743e503>:158: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In iteration: 1\n",
      "After 0 steps, perplexity is 9986.188\n",
      "After 100 steps, perplexity is 1484.217\n",
      "After 200 steps, perplexity is 1098.021\n",
      "After 300 steps, perplexity is 912.162\n",
      "After 400 steps, perplexity is 796.410\n",
      "After 500 steps, perplexity is 717.616\n",
      "After 600 steps, perplexity is 658.244\n",
      "After 700 steps, perplexity is 607.075\n",
      "After 800 steps, perplexity is 561.098\n",
      "After 900 steps, perplexity is 525.234\n",
      "After 1000 steps, perplexity is 497.539\n",
      "After 1100 steps, perplexity is 471.414\n",
      "After 1200 steps, perplexity is 449.789\n",
      "After 1300 steps, perplexity is 430.604\n",
      "Epoch: 1 Validation Perplexity: 248.431\n",
      "In iteration: 2\n",
      "After 0 steps, perplexity is 356.195\n",
      "After 100 steps, perplexity is 245.642\n",
      "After 200 steps, perplexity is 250.828\n",
      "After 300 steps, perplexity is 251.560\n",
      "After 400 steps, perplexity is 248.274\n",
      "After 500 steps, perplexity is 245.453\n",
      "After 600 steps, perplexity is 244.791\n",
      "After 700 steps, perplexity is 242.148\n",
      "After 800 steps, perplexity is 237.386\n",
      "After 900 steps, perplexity is 234.582\n",
      "After 1000 steps, perplexity is 232.804\n",
      "After 1100 steps, perplexity is 229.277\n",
      "After 1200 steps, perplexity is 226.668\n",
      "After 1300 steps, perplexity is 223.875\n",
      "Epoch: 2 Validation Perplexity: 182.171\n",
      "Test Perplexity: 177.237\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import reader\n",
    "\n",
    "DATA_PATH = \"simple-examples/data/\"\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "HIDDEN_SIZE = 200\n",
    "NUM_LAYERS = 2\n",
    "LEARNING_RATE = 1.0\n",
    "KEEP_PROB = 0.5\n",
    "MAX_GRAD_NORM = 5\n",
    "\n",
    "TRAIN_BATCH_SIZE = 20\n",
    "TRAIN_NUM_STEP = 35\n",
    "\n",
    "EVAL_BATCH_SIZE = 1\n",
    "EVAL_NUM_STEP = 1\n",
    "NUM_EPOCH = 2\n",
    "\n",
    "class PTBModel:\n",
    "    def __init__(self, is_training, batch_size, num_steps):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps  = num_steps\n",
    "       \n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        self.targets    = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "      \n",
    "        stack_rnn = []\n",
    "        for i in range(NUM_LAYERS):\n",
    "            lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units = HIDDEN_SIZE, state_is_tuple = True, reuse=tf.get_variable_scope().reuse)\n",
    "            if is_training:\n",
    "                lstm_cell = tf.contrib.rnn.DropoutWrapper (lstm_cell, output_keep_prob = KEEP_PROB)\n",
    "            stack_rnn.append(lstm_cell)\n",
    "            \n",
    "            \n",
    "        cell = tf.contrib.rnn.MultiRNNCell(stack_rnn, state_is_tuple = True)\n",
    "            \n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        embedding = tf.get_variable(\"embedding\", [VOCAB_SIZE, HIDDEN_SIZE])\n",
    "        # map input data integer into a 1 * HIDDEN_SIZE vector\n",
    "        # that means input becomes variables.\n",
    "        inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "        # inputs shape is batch size * num_steps * HIDDEN_SIZE\n",
    "        if is_training:\n",
    "            inputs = tf.nn.dropout(inputs, KEEP_PROB)\n",
    "        \n",
    "        outputs = []\n",
    "        state = self.initial_state\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0: \n",
    "                    tf.get_variable_scope().reuse_variables() # to make sure cell_output, state can be reused.\n",
    "                cell_output, state = cell(inputs[:, time_step, :], state)\n",
    "                outputs.append(cell_output)\n",
    "        \n",
    "        #print(outputs[0], len(outputs))\n",
    "        output = tf.reshape(tf.concat(outputs, 1), [-1, HIDDEN_SIZE])\n",
    "        # merge different batchs to form (batch_num * num_steps) * HIDDEN_SIZE\n",
    "        #print(output.shape)\n",
    "\n",
    "        # softmax layer\n",
    "        softmax_weight = tf.get_variable(\"softmax_w\", [HIDDEN_SIZE, VOCAB_SIZE])\n",
    "        softmax_bias   = tf.get_variable(\"softmax_b\", [VOCAB_SIZE])\n",
    "\n",
    "        # loss\n",
    "        logits = tf.matmul(output, softmax_weight) + softmax_bias # becomes one-hot representation\n",
    "\n",
    "        #print(self.targets.shape)\n",
    "        #print(logits.shape)\n",
    "        \n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "            [logits], # (batch_num * num_steps) * HIDDEN_SIZE\n",
    "            [tf.reshape(self.targets, [-1])], # (batch_num * num_steps) * 1 \n",
    "            [tf.ones([batch_size * num_steps], dtype=tf.float32)]  # weight in each point in batch_size * num_steps.\n",
    "            )\n",
    "        self.cost = tf.reduce_sum(loss) / batch_size\n",
    "        self.final_state = state\n",
    "\n",
    "        if not is_training: return\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        # get all variables for training\n",
    "        #print(trainable_variables)\n",
    "\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, trainable_variables), MAX_GRAD_NORM)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)\n",
    "\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "        \n",
    "        \n",
    "def run_epoch(session, model, data, train_op, output_log, epoch_size):\n",
    "    total_costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(model.initial_state)\n",
    "       \n",
    "    for step in range(epoch_size):\n",
    "        x, y =session.run(data)\n",
    "        # for example: x = [ 32, 3, 44, 55, 66, 4554, 554 ], then y = [3, 44, 55, 66, 4554, 554, xxxx]\n",
    "        # print(x.shape)\n",
    "        cost, state, _ = session.run([model.cost, model.final_state, train_op], {model.input_data: x, model.targets: y, model.initial_state: state})\n",
    "        total_costs += cost\n",
    "        iters += model.num_steps\n",
    "        \n",
    "        if output_log and step % 100 == 0:\n",
    "            print(\"After %d steps, perplexity is %.3f\"%(step, np.exp(total_costs / iters)))\n",
    "            \n",
    "    return np.exp(total_costs / iters)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "def main():\n",
    "    train_data, valid_data, test_data, _ = reader.ptb_raw_data(DATA_PATH)\n",
    "    # the basic idea in ptb_raw_data is that we obtain words and do counts for each word, replace words by its sorted index\n",
    "    # for example, (a, d, a ,b, a, c, d)=> {a:3, d:2, b:1, c:1} => a is replaced by 0, d = 1, b = 2, c = 3\n",
    "    #print(train_data[1:200])\n",
    "    print(tf.__version__)\n",
    "    train_data_len   = len(train_data)\n",
    "    train_batch_len  = train_data_len // TRAIN_BATCH_SIZE\n",
    "    train_epoch_size = (train_batch_len - 1) // TRAIN_NUM_STEP \n",
    "    \n",
    "    #print(train_data_len, train_batch_len, train_epoch_size)\n",
    "    \n",
    "    valid_data_len   = len(valid_data)\n",
    "    valid_batch_len  = valid_data_len // EVAL_BATCH_SIZE\n",
    "    valid_epoch_size = (valid_batch_len - 1) // EVAL_NUM_STEP \n",
    "    \n",
    "    #print(valid_data_len, train_batch_len, valid_epoch_size)\n",
    "    \n",
    "    test_data_len   = len(test_data)\n",
    "    test_batch_len  = test_data_len // EVAL_BATCH_SIZE\n",
    "    test_epoch_size = (test_batch_len - 1) // EVAL_NUM_STEP \n",
    "    \n",
    "    #print(test_data_len, test_batch_len, test_epoch_size)\n",
    "    \n",
    "    initializer = tf.random_uniform_initializer(-0.05, 0.05) \n",
    "    # use for variables initialization in tf scope.\n",
    "    #print(initializer)\n",
    "    \n",
    "    with tf.variable_scope(\"language_model\", reuse=tf.AUTO_REUSE, initializer=initializer):\n",
    "        train_model = PTBModel(True, TRAIN_BATCH_SIZE, TRAIN_NUM_STEP)\n",
    "       \n",
    "    with tf.variable_scope(\"language_model\", reuse=True, initializer=initializer):\n",
    "        eval_model  = PTBModel(False, EVAL_BATCH_SIZE, EVAL_NUM_STEP)\n",
    "    \n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        train_queue = reader.ptb_producer(train_data, train_model.batch_size, train_model.num_steps)\n",
    "        eval_queue  = reader.ptb_producer(valid_data, eval_model.batch_size,  eval_model.num_steps)\n",
    "        test_queue  = reader.ptb_producer(test_data,  eval_model.batch_size,  eval_model.num_steps)\n",
    "        \n",
    "        #print(train_queue)\n",
    "    \n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=session, coord=coord)\n",
    "        # not clear here. each queue(train queue, eval queue, test queue) has its own thread???\n",
    "        \n",
    "        \n",
    "        for i in range(NUM_EPOCH):\n",
    "            print(\"In iteration: %d\" % (i+1))\n",
    "            run_epoch(session, train_model, train_queue, train_model.train_op, True, train_epoch_size)\n",
    "    \n",
    "            valid_perplexity = run_epoch(session, eval_model, eval_queue, tf.no_op(), False, valid_epoch_size)\n",
    "        \n",
    "            print(\"Epoch: %d Validation Perplexity: %.3f\" % (i+1, valid_perplexity))\n",
    "    \n",
    "        \n",
    "        test_perplexity = run_epoch(session, eval_model, test_queue, tf.no_op(), False, test_epoch_size)\n",
    "        print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "        \n",
    "        \n",
    "        coord.request_stop()   # request stop multi-thread\n",
    "        coord.join(threads)    # wait for all threads stops\n",
    "        \n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度问题\n",
    "了解RNN大概过程后，这里提出一个神经网络存在的的经典问题，梯度问题\n",
    "梯度爆炸 (梯度值太大， 调参没有意义)\n",
    "如何确定是否出现梯度爆炸？\n",
    "           训练过程中出现梯度爆炸会伴随一些细微的信号，如：\n",
    "\n",
    "           模型无法从训练数据中获得更新（如低损失）。\n",
    "\n",
    "           模型不稳定，导致更新过程中的损失出现显著变化。\n",
    "\n",
    "           训练过程中，模型损失变成NaN。\n",
    "解决方案：\n",
    "梯度剪切这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。\n",
    "\n",
    "权重正则化（weithts regularization）比较常见的是l1l1正则，和l2l2正则，在各个深度框架中都有相应的API可以使用正则化，\n",
    "relu、leakrelu、elu等激活函数\n",
    "注：事实上，在深度神经网络中，往往是梯度消失出现的更多一些。\n",
    "\n",
    "梯度消失：(梯度接近于0， 调参没有方向感)\n",
    "\n",
    "由前文可见，RNN可以带上记忆。假设，一个生成下一个单词的例子：“这顿饭真好”->“吃”，很明显，我们只要前5个字就能猜到下一个字是什么了。当只有一个好的时候，没办法知道是好吃还是好玩还是好什么，只有当记忆连带了5个历史的是时候，这顿饭真、、、，知道了饭才知道好后面应该跟着吃，这就是为什么rnn带上记忆之后能在文本生成上进行一个很好的处理。\n",
    "\n",
    "不过这里还有一个问题就是，如果这个长度要够长的话你才能知道前文信息有多少，只有足够长足够长才能存储够多的记忆，比如有个人跟你聊天聊了一整天，上午的时候问了你一个笑话，下午的时候问你，诶穿山甲说了什么？能回答吗，或者说几天之后问的的呢，显然不能回答。如果说这个rnn不能记忆到几天前的s的话，那么其实这个处理能力时为零的，因为它还是不知道你的上下文前提是什么。也就是说，它和bp一样，都有一个梯度消失的问题，很难学习到长期的依赖，随着传播和时间的流逝不断的衰减，第一次传入的时候对决策的影响较大，传到第二个的时候变的比较小，第三个更小，经过5到6次的传播，对决策基本上没什么作用了。\n",
    "\n",
    "在求梯度的时候，矩阵中存在比较小的值，多个矩阵相乘会使梯度值以指数级速度下降，最终在几步后完全消失。比较远的时刻的梯度值为0，这些时刻的状态对学习过程没有帮助，导致你无法学习到长距离依赖。消失梯度问题不仅出现在RNN中，同样也出现在深度前向神经网络中。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
